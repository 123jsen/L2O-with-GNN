{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "This file trains `num_optimizers` LSTM and GNN optimizers, use them to perform certain tasks, and then compare the statistics of their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from trainUtil.trainingLSTM import trainLSTM\n",
    "from trainUtil.trainingGNN import trainGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "num_optimizers = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training lstm-0\n",
      "0-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.3557214736938477\n",
      "Batch 125 / 469, Model loss: 0.6286089420318604\n",
      "Batch 250 / 469, Model loss: 0.42230042815208435\n",
      "Batch 375 / 469, Model loss: 0.2537669241428375\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.29018959403038025\n",
      "Batch 125 / 469, Model loss: 0.41859161853790283\n",
      "Batch 250 / 469, Model loss: 0.36551350355148315\n",
      "Batch 375 / 469, Model loss: 0.27287986874580383\n",
      "1-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.299006938934326\n",
      "Batch 125 / 469, Model loss: 0.5245992541313171\n",
      "Batch 250 / 469, Model loss: 0.36455240845680237\n",
      "Batch 375 / 469, Model loss: 0.23145566880702972\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.24291564524173737\n",
      "Batch 125 / 469, Model loss: 0.3329777121543884\n",
      "Batch 250 / 469, Model loss: 0.2829943597316742\n",
      "Batch 375 / 469, Model loss: 0.18678027391433716\n",
      "2-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.296865940093994\n",
      "Batch 125 / 469, Model loss: 0.8152872323989868\n",
      "Batch 250 / 469, Model loss: 0.44990381598472595\n",
      "Batch 375 / 469, Model loss: 0.2971303164958954\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.2904365360736847\n",
      "Batch 125 / 469, Model loss: 0.38830453157424927\n",
      "Batch 250 / 469, Model loss: 0.35625648498535156\n",
      "Batch 375 / 469, Model loss: 0.21561390161514282\n",
      "Training gnn-0\n",
      "0-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.3327481746673584\n",
      "Batch 125 / 469, Model loss: 0.6318775415420532\n",
      "Batch 250 / 469, Model loss: 0.40209007263183594\n",
      "Batch 375 / 469, Model loss: 0.24803608655929565\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.23501653969287872\n",
      "Batch 125 / 469, Model loss: 0.3502843677997589\n",
      "Batch 250 / 469, Model loss: 0.33829107880592346\n",
      "Batch 375 / 469, Model loss: 0.2071342170238495\n",
      "1-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.3298683166503906\n",
      "Batch 125 / 469, Model loss: 0.871172308921814\n",
      "Batch 250 / 469, Model loss: 0.4222741425037384\n",
      "Batch 375 / 469, Model loss: 0.29131844639778137\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.294400691986084\n",
      "Batch 125 / 469, Model loss: 0.3630765676498413\n",
      "Batch 250 / 469, Model loss: 0.3340897262096405\n",
      "Batch 375 / 469, Model loss: 0.2111411839723587\n",
      "2-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.3354856967926025\n",
      "Batch 125 / 469, Model loss: 0.7769808173179626\n",
      "Batch 250 / 469, Model loss: 0.45144838094711304\n",
      "Batch 375 / 469, Model loss: 0.2859816253185272\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.29768839478492737\n",
      "Batch 125 / 469, Model loss: 0.383820503950119\n",
      "Batch 250 / 469, Model loss: 0.3551889657974243\n",
      "Batch 375 / 469, Model loss: 0.21720191836357117\n",
      "Training lstm-1\n",
      "0-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.3089725971221924\n",
      "Batch 125 / 469, Model loss: 0.7676549553871155\n",
      "Batch 250 / 469, Model loss: 0.4417959153652191\n",
      "Batch 375 / 469, Model loss: 0.28068313002586365\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.31114354729652405\n",
      "Batch 125 / 469, Model loss: 0.37598633766174316\n",
      "Batch 250 / 469, Model loss: 0.3393031358718872\n",
      "Batch 375 / 469, Model loss: 0.20758071541786194\n",
      "1-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.316037893295288\n",
      "Batch 125 / 469, Model loss: 0.8726287484169006\n",
      "Batch 250 / 469, Model loss: 0.4622543454170227\n",
      "Batch 375 / 469, Model loss: 0.27219241857528687\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.3006768226623535\n",
      "Batch 125 / 469, Model loss: 0.3760419487953186\n",
      "Batch 250 / 469, Model loss: 0.34957438707351685\n",
      "Batch 375 / 469, Model loss: 0.20153282582759857\n",
      "2-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.28977108001709\n",
      "Batch 125 / 469, Model loss: 0.7700207829475403\n",
      "Batch 250 / 469, Model loss: 0.45429766178131104\n",
      "Batch 375 / 469, Model loss: 0.2718566358089447\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.2842411696910858\n",
      "Batch 125 / 469, Model loss: 0.37517088651657104\n",
      "Batch 250 / 469, Model loss: 0.35891860723495483\n",
      "Batch 375 / 469, Model loss: 0.1983705461025238\n",
      "Training gnn-1\n",
      "0-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.2963573932647705\n",
      "Batch 125 / 469, Model loss: 0.7408007383346558\n",
      "Batch 250 / 469, Model loss: 0.46473556756973267\n",
      "Batch 375 / 469, Model loss: 0.2937389314174652\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.2918950617313385\n",
      "Batch 125 / 469, Model loss: 0.39081642031669617\n",
      "Batch 250 / 469, Model loss: 0.37080419063568115\n",
      "Batch 375 / 469, Model loss: 0.21965184807777405\n",
      "1-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.329406261444092\n",
      "Batch 125 / 469, Model loss: 0.7825015783309937\n",
      "Batch 250 / 469, Model loss: 0.44496071338653564\n",
      "Batch 375 / 469, Model loss: 0.28402429819107056\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.31496113538742065\n",
      "Batch 125 / 469, Model loss: 0.3947559893131256\n",
      "Batch 250 / 469, Model loss: 0.34730905294418335\n",
      "Batch 375 / 469, Model loss: 0.21294358372688293\n",
      "2-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.320254325866699\n",
      "Batch 125 / 469, Model loss: 0.7042170763015747\n",
      "Batch 250 / 469, Model loss: 0.43111661076545715\n",
      "Batch 375 / 469, Model loss: 0.2705470025539398\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.28331705927848816\n",
      "Batch 125 / 469, Model loss: 0.38394513726234436\n",
      "Batch 250 / 469, Model loss: 0.3344038724899292\n",
      "Batch 375 / 469, Model loss: 0.20136089622974396\n",
      "Training lstm-2\n",
      "0-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.2948174476623535\n",
      "Batch 125 / 469, Model loss: 0.8067861795425415\n",
      "Batch 250 / 469, Model loss: 0.45155060291290283\n",
      "Batch 375 / 469, Model loss: 0.28813356161117554\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.2774480879306793\n",
      "Batch 125 / 469, Model loss: 0.3745083808898926\n",
      "Batch 250 / 469, Model loss: 0.34901243448257446\n",
      "Batch 375 / 469, Model loss: 0.21178971230983734\n",
      "1-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.282846450805664\n",
      "Batch 125 / 469, Model loss: 0.7822891473770142\n",
      "Batch 250 / 469, Model loss: 0.43675339221954346\n",
      "Batch 375 / 469, Model loss: 0.2783379554748535\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.2897990047931671\n",
      "Batch 125 / 469, Model loss: 0.383173406124115\n",
      "Batch 250 / 469, Model loss: 0.3572155237197876\n",
      "Batch 375 / 469, Model loss: 0.20688219368457794\n",
      "2-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.3002960681915283\n",
      "Batch 125 / 469, Model loss: 0.9058278799057007\n",
      "Batch 250 / 469, Model loss: 0.47665655612945557\n",
      "Batch 375 / 469, Model loss: 0.29426535964012146\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.3059997260570526\n",
      "Batch 125 / 469, Model loss: 0.40638792514801025\n",
      "Batch 250 / 469, Model loss: 0.3598042130470276\n",
      "Batch 375 / 469, Model loss: 0.20721568167209625\n",
      "Training gnn-2\n",
      "0-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.332275629043579\n",
      "Batch 125 / 469, Model loss: 0.7555601596832275\n",
      "Batch 250 / 469, Model loss: 0.4169022738933563\n",
      "Batch 375 / 469, Model loss: 0.2829956114292145\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.2823735773563385\n",
      "Batch 125 / 469, Model loss: 0.3956776261329651\n",
      "Batch 250 / 469, Model loss: 0.33565038442611694\n",
      "Batch 375 / 469, Model loss: 0.2102229744195938\n",
      "1-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.3327128887176514\n",
      "Batch 125 / 469, Model loss: 0.7608673572540283\n",
      "Batch 250 / 469, Model loss: 0.4351029396057129\n",
      "Batch 375 / 469, Model loss: 0.28712648153305054\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.29722893238067627\n",
      "Batch 125 / 469, Model loss: 0.39751380681991577\n",
      "Batch 250 / 469, Model loss: 0.3452027142047882\n",
      "Batch 375 / 469, Model loss: 0.21253523230552673\n",
      "2-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.2972469329833984\n",
      "Batch 125 / 469, Model loss: 0.6996668577194214\n",
      "Batch 250 / 469, Model loss: 0.4442269504070282\n",
      "Batch 375 / 469, Model loss: 0.28100156784057617\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.29146137833595276\n",
      "Batch 125 / 469, Model loss: 0.4027240574359894\n",
      "Batch 250 / 469, Model loss: 0.3516225516796112\n",
      "Batch 375 / 469, Model loss: 0.2047816663980484\n",
      "Training lstm-3\n",
      "0-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.3118479251861572\n",
      "Batch 125 / 469, Model loss: 0.7957957983016968\n",
      "Batch 250 / 469, Model loss: 0.4558711647987366\n",
      "Batch 375 / 469, Model loss: 0.284602552652359\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.29030466079711914\n",
      "Batch 125 / 469, Model loss: 0.3928905427455902\n",
      "Batch 250 / 469, Model loss: 0.36075854301452637\n",
      "Batch 375 / 469, Model loss: 0.2090759575366974\n",
      "1-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.298898935317993\n",
      "Batch 125 / 469, Model loss: 0.8199217915534973\n",
      "Batch 250 / 469, Model loss: 0.46199050545692444\n",
      "Batch 375 / 469, Model loss: 0.28604811429977417\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.30151474475860596\n",
      "Batch 125 / 469, Model loss: 0.394869863986969\n",
      "Batch 250 / 469, Model loss: 0.3511844873428345\n",
      "Batch 375 / 469, Model loss: 0.2029111534357071\n",
      "2-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.2915937900543213\n",
      "Batch 125 / 469, Model loss: 0.7663424015045166\n",
      "Batch 250 / 469, Model loss: 0.4507562220096588\n",
      "Batch 375 / 469, Model loss: 0.2906630337238312\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.28755074739456177\n",
      "Batch 125 / 469, Model loss: 0.3818642795085907\n",
      "Batch 250 / 469, Model loss: 0.34505951404571533\n",
      "Batch 375 / 469, Model loss: 0.2066129893064499\n",
      "Training gnn-3\n",
      "0-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.3145651817321777\n",
      "Batch 125 / 469, Model loss: 0.7643250823020935\n",
      "Batch 250 / 469, Model loss: 0.4190613627433777\n",
      "Batch 375 / 469, Model loss: 0.27839720249176025\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.282023549079895\n",
      "Batch 125 / 469, Model loss: 0.41488081216812134\n",
      "Batch 250 / 469, Model loss: 0.332522988319397\n",
      "Batch 375 / 469, Model loss: 0.20314942300319672\n",
      "1-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.31066632270813\n",
      "Batch 125 / 469, Model loss: 0.7801327705383301\n",
      "Batch 250 / 469, Model loss: 0.43310657143592834\n",
      "Batch 375 / 469, Model loss: 0.2956433594226837\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.3063812553882599\n",
      "Batch 125 / 469, Model loss: 0.3942077159881592\n",
      "Batch 250 / 469, Model loss: 0.3452679514884949\n",
      "Batch 375 / 469, Model loss: 0.22177262604236603\n",
      "2-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.318615436553955\n",
      "Batch 125 / 469, Model loss: 0.772781491279602\n",
      "Batch 250 / 469, Model loss: 0.43736201524734497\n",
      "Batch 375 / 469, Model loss: 0.27993836998939514\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.2867743670940399\n",
      "Batch 125 / 469, Model loss: 0.39141011238098145\n",
      "Batch 250 / 469, Model loss: 0.3526016175746918\n",
      "Batch 375 / 469, Model loss: 0.21248406171798706\n",
      "Training lstm-4\n",
      "0-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.288709878921509\n",
      "Batch 125 / 469, Model loss: 0.8023948073387146\n",
      "Batch 250 / 469, Model loss: 0.4496832489967346\n",
      "Batch 375 / 469, Model loss: 0.2788287401199341\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.3017868101596832\n",
      "Batch 125 / 469, Model loss: 0.38140493631362915\n",
      "Batch 250 / 469, Model loss: 0.34254369139671326\n",
      "Batch 375 / 469, Model loss: 0.20666572451591492\n",
      "1-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.3228952884674072\n",
      "Batch 125 / 469, Model loss: 0.8444733619689941\n",
      "Batch 250 / 469, Model loss: 0.4511587917804718\n",
      "Batch 375 / 469, Model loss: 0.2829534113407135\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.28852418065071106\n",
      "Batch 125 / 469, Model loss: 0.36378782987594604\n",
      "Batch 250 / 469, Model loss: 0.3340507745742798\n",
      "Batch 375 / 469, Model loss: 0.2086542397737503\n",
      "2-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.311668872833252\n",
      "Batch 125 / 469, Model loss: 0.7371853590011597\n",
      "Batch 250 / 469, Model loss: 0.43620553612709045\n",
      "Batch 375 / 469, Model loss: 0.274827778339386\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.28346362709999084\n",
      "Batch 125 / 469, Model loss: 0.3652798533439636\n",
      "Batch 250 / 469, Model loss: 0.33408409357070923\n",
      "Batch 375 / 469, Model loss: 0.20772206783294678\n",
      "Training gnn-4\n",
      "0-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.3085432052612305\n",
      "Batch 125 / 469, Model loss: 0.7026321887969971\n",
      "Batch 250 / 469, Model loss: 0.44296473264694214\n",
      "Batch 375 / 469, Model loss: 0.2869434654712677\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.298885703086853\n",
      "Batch 125 / 469, Model loss: 0.4125477075576782\n",
      "Batch 250 / 469, Model loss: 0.34650179743766785\n",
      "Batch 375 / 469, Model loss: 0.21702000498771667\n",
      "1-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.2994542121887207\n",
      "Batch 125 / 469, Model loss: 0.7163777947425842\n",
      "Batch 250 / 469, Model loss: 0.4519282877445221\n",
      "Batch 375 / 469, Model loss: 0.2825769782066345\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.2975919544696808\n",
      "Batch 125 / 469, Model loss: 0.3972378373146057\n",
      "Batch 250 / 469, Model loss: 0.34039127826690674\n",
      "Batch 375 / 469, Model loss: 0.213809996843338\n",
      "2-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.3123350143432617\n",
      "Batch 125 / 469, Model loss: 0.7348950505256653\n",
      "Batch 250 / 469, Model loss: 0.42895790934562683\n",
      "Batch 375 / 469, Model loss: 0.28763484954833984\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.2941950559616089\n",
      "Batch 125 / 469, Model loss: 0.40420445799827576\n",
      "Batch 250 / 469, Model loss: 0.34324732422828674\n",
      "Batch 375 / 469, Model loss: 0.21139825880527496\n",
      "Training lstm-5\n",
      "0-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.3072149753570557\n",
      "Batch 125 / 469, Model loss: 0.7483788728713989\n",
      "Batch 250 / 469, Model loss: 0.43822646141052246\n",
      "Batch 375 / 469, Model loss: 0.2751626670360565\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.2832980751991272\n",
      "Batch 125 / 469, Model loss: 0.37446293234825134\n",
      "Batch 250 / 469, Model loss: 0.34284448623657227\n",
      "Batch 375 / 469, Model loss: 0.20136696100234985\n",
      "1-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.3319737911224365\n",
      "Batch 125 / 469, Model loss: 0.7018125057220459\n",
      "Batch 250 / 469, Model loss: 0.44709306955337524\n",
      "Batch 375 / 469, Model loss: 0.2738930583000183\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.2914091646671295\n",
      "Batch 125 / 469, Model loss: 0.3780460059642792\n",
      "Batch 250 / 469, Model loss: 0.335945188999176\n",
      "Batch 375 / 469, Model loss: 0.20618797838687897\n",
      "2-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.30063533782959\n",
      "Batch 125 / 469, Model loss: 0.6952720284461975\n",
      "Batch 250 / 469, Model loss: 0.43137574195861816\n",
      "Batch 375 / 469, Model loss: 0.27716901898384094\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.28237730264663696\n",
      "Batch 125 / 469, Model loss: 0.3667331337928772\n",
      "Batch 250 / 469, Model loss: 0.3400762379169464\n",
      "Batch 375 / 469, Model loss: 0.2037130445241928\n",
      "Training gnn-5\n",
      "0-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.3005752563476562\n",
      "Batch 125 / 469, Model loss: 0.7460526823997498\n",
      "Batch 250 / 469, Model loss: 0.4563939571380615\n",
      "Batch 375 / 469, Model loss: 0.2829248905181885\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.29572078585624695\n",
      "Batch 125 / 469, Model loss: 0.4011467695236206\n",
      "Batch 250 / 469, Model loss: 0.3521445393562317\n",
      "Batch 375 / 469, Model loss: 0.21555732190608978\n",
      "1-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.2911744117736816\n",
      "Batch 125 / 469, Model loss: 0.731837809085846\n",
      "Batch 250 / 469, Model loss: 0.45328137278556824\n",
      "Batch 375 / 469, Model loss: 0.27415117621421814\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.3012237548828125\n",
      "Batch 125 / 469, Model loss: 0.38461166620254517\n",
      "Batch 250 / 469, Model loss: 0.3680143356323242\n",
      "Batch 375 / 469, Model loss: 0.19639316201210022\n",
      "2-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.298403024673462\n",
      "Batch 125 / 469, Model loss: 0.7632666826248169\n",
      "Batch 250 / 469, Model loss: 0.4538012444972992\n",
      "Batch 375 / 469, Model loss: 0.2754786014556885\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.31276440620422363\n",
      "Batch 125 / 469, Model loss: 0.39399832487106323\n",
      "Batch 250 / 469, Model loss: 0.3520754277706146\n",
      "Batch 375 / 469, Model loss: 0.21184854209423065\n",
      "Training lstm-6\n",
      "0-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.305966854095459\n",
      "Batch 125 / 469, Model loss: 0.810653805732727\n",
      "Batch 250 / 469, Model loss: 0.4528005123138428\n",
      "Batch 375 / 469, Model loss: 0.2929396629333496\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.29532933235168457\n",
      "Batch 125 / 469, Model loss: 0.378397673368454\n",
      "Batch 250 / 469, Model loss: 0.3437029719352722\n",
      "Batch 375 / 469, Model loss: 0.2146185040473938\n",
      "1-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.30130672454834\n",
      "Batch 125 / 469, Model loss: 0.8929177522659302\n",
      "Batch 250 / 469, Model loss: 0.4735110402107239\n",
      "Batch 375 / 469, Model loss: 0.2946646213531494\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.30021098256111145\n",
      "Batch 125 / 469, Model loss: 0.3915784955024719\n",
      "Batch 250 / 469, Model loss: 0.35910600423812866\n",
      "Batch 375 / 469, Model loss: 0.20702588558197021\n",
      "2-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.3279764652252197\n",
      "Batch 125 / 469, Model loss: 0.7826290130615234\n",
      "Batch 250 / 469, Model loss: 0.4577488303184509\n",
      "Batch 375 / 469, Model loss: 0.2855184078216553\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.29036465287208557\n",
      "Batch 125 / 469, Model loss: 0.37330707907676697\n",
      "Batch 250 / 469, Model loss: 0.34362277388572693\n",
      "Batch 375 / 469, Model loss: 0.2160712033510208\n",
      "Training gnn-6\n",
      "0-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.3016676902770996\n",
      "Batch 125 / 469, Model loss: 0.673366367816925\n",
      "Batch 250 / 469, Model loss: 0.4253900647163391\n",
      "Batch 375 / 469, Model loss: 0.2797146141529083\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.2920592427253723\n",
      "Batch 125 / 469, Model loss: 0.39224866032600403\n",
      "Batch 250 / 469, Model loss: 0.34309759736061096\n",
      "Batch 375 / 469, Model loss: 0.2051011025905609\n",
      "1-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.301150321960449\n",
      "Batch 125 / 469, Model loss: 0.7762535810470581\n",
      "Batch 250 / 469, Model loss: 0.4380725026130676\n",
      "Batch 375 / 469, Model loss: 0.2681271731853485\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.29892411828041077\n",
      "Batch 125 / 469, Model loss: 0.39318937063217163\n",
      "Batch 250 / 469, Model loss: 0.32904401421546936\n",
      "Batch 375 / 469, Model loss: 0.19199314713478088\n",
      "2-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.2879958152770996\n",
      "Batch 125 / 469, Model loss: 0.731002151966095\n",
      "Batch 250 / 469, Model loss: 0.444359689950943\n",
      "Batch 375 / 469, Model loss: 0.2845441997051239\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.28960099816322327\n",
      "Batch 125 / 469, Model loss: 0.3887471556663513\n",
      "Batch 250 / 469, Model loss: 0.3418741524219513\n",
      "Batch 375 / 469, Model loss: 0.20592334866523743\n",
      "Training lstm-7\n",
      "0-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.3137662410736084\n",
      "Batch 125 / 469, Model loss: 0.7182524800300598\n",
      "Batch 250 / 469, Model loss: 0.44449564814567566\n",
      "Batch 375 / 469, Model loss: 0.2784353196620941\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.2838636040687561\n",
      "Batch 125 / 469, Model loss: 0.3801477253437042\n",
      "Batch 250 / 469, Model loss: 0.35993000864982605\n",
      "Batch 375 / 469, Model loss: 0.20392416417598724\n",
      "1-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.317584753036499\n",
      "Batch 125 / 469, Model loss: 0.8440988063812256\n",
      "Batch 250 / 469, Model loss: 0.44503480195999146\n",
      "Batch 375 / 469, Model loss: 0.29352855682373047\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.28884103894233704\n",
      "Batch 125 / 469, Model loss: 0.3813156187534332\n",
      "Batch 250 / 469, Model loss: 0.3327619731426239\n",
      "Batch 375 / 469, Model loss: 0.211941659450531\n",
      "2-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.307257652282715\n",
      "Batch 125 / 469, Model loss: 0.68846195936203\n",
      "Batch 250 / 469, Model loss: 0.441368043422699\n",
      "Batch 375 / 469, Model loss: 0.269113153219223\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.2796768844127655\n",
      "Batch 125 / 469, Model loss: 0.38179636001586914\n",
      "Batch 250 / 469, Model loss: 0.3497990667819977\n",
      "Batch 375 / 469, Model loss: 0.2029588520526886\n",
      "Training gnn-7\n",
      "0-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.320974349975586\n",
      "Batch 125 / 469, Model loss: 0.8142883777618408\n",
      "Batch 250 / 469, Model loss: 0.437202125787735\n",
      "Batch 375 / 469, Model loss: 0.28272515535354614\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.29361626505851746\n",
      "Batch 125 / 469, Model loss: 0.3969374895095825\n",
      "Batch 250 / 469, Model loss: 0.3340134918689728\n",
      "Batch 375 / 469, Model loss: 0.2064446210861206\n",
      "1-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.309701681137085\n",
      "Batch 125 / 469, Model loss: 0.7314116358757019\n",
      "Batch 250 / 469, Model loss: 0.4230845272541046\n",
      "Batch 375 / 469, Model loss: 0.2727430760860443\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.27940380573272705\n",
      "Batch 125 / 469, Model loss: 0.36899617314338684\n",
      "Batch 250 / 469, Model loss: 0.3347514867782593\n",
      "Batch 375 / 469, Model loss: 0.19005824625492096\n",
      "2-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.3069241046905518\n",
      "Batch 125 / 469, Model loss: 0.7771358489990234\n",
      "Batch 250 / 469, Model loss: 0.43775278329849243\n",
      "Batch 375 / 469, Model loss: 0.2724236249923706\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.2855314016342163\n",
      "Batch 125 / 469, Model loss: 0.3689509332180023\n",
      "Batch 250 / 469, Model loss: 0.32356297969818115\n",
      "Batch 375 / 469, Model loss: 0.19670820236206055\n",
      "Training lstm-8\n",
      "0-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.2912096977233887\n",
      "Batch 125 / 469, Model loss: 0.8240805268287659\n",
      "Batch 250 / 469, Model loss: 0.44644176959991455\n",
      "Batch 375 / 469, Model loss: 0.2884286940097809\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.29792773723602295\n",
      "Batch 125 / 469, Model loss: 0.38598912954330444\n",
      "Batch 250 / 469, Model loss: 0.34165334701538086\n",
      "Batch 375 / 469, Model loss: 0.2065112143754959\n",
      "1-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.2892794609069824\n",
      "Batch 125 / 469, Model loss: 0.8369821310043335\n",
      "Batch 250 / 469, Model loss: 0.45619693398475647\n",
      "Batch 375 / 469, Model loss: 0.2856385111808777\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.30292993783950806\n",
      "Batch 125 / 469, Model loss: 0.41070815920829773\n",
      "Batch 250 / 469, Model loss: 0.3465200960636139\n",
      "Batch 375 / 469, Model loss: 0.21325469017028809\n",
      "2-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.3155269622802734\n",
      "Batch 125 / 469, Model loss: 0.7921053171157837\n",
      "Batch 250 / 469, Model loss: 0.43383681774139404\n",
      "Batch 375 / 469, Model loss: 0.285856693983078\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.2935774624347687\n",
      "Batch 125 / 469, Model loss: 0.38239455223083496\n",
      "Batch 250 / 469, Model loss: 0.3357974886894226\n",
      "Batch 375 / 469, Model loss: 0.21072915196418762\n",
      "Training gnn-8\n",
      "0-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.30912709236145\n",
      "Batch 125 / 469, Model loss: 0.7395843863487244\n",
      "Batch 250 / 469, Model loss: 0.4393019378185272\n",
      "Batch 375 / 469, Model loss: 0.28376874327659607\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.2886219024658203\n",
      "Batch 125 / 469, Model loss: 0.3837273120880127\n",
      "Batch 250 / 469, Model loss: 0.3230277895927429\n",
      "Batch 375 / 469, Model loss: 0.2113044112920761\n",
      "1-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.2823338508605957\n",
      "Batch 125 / 469, Model loss: 0.7210463881492615\n",
      "Batch 250 / 469, Model loss: 0.4550815522670746\n",
      "Batch 375 / 469, Model loss: 0.27618682384490967\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.2934874892234802\n",
      "Batch 125 / 469, Model loss: 0.3859921991825104\n",
      "Batch 250 / 469, Model loss: 0.3535935878753662\n",
      "Batch 375 / 469, Model loss: 0.20599791407585144\n",
      "2-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.292452812194824\n",
      "Batch 125 / 469, Model loss: 0.8212736248970032\n",
      "Batch 250 / 469, Model loss: 0.46375399827957153\n",
      "Batch 375 / 469, Model loss: 0.2877342104911804\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.30858784914016724\n",
      "Batch 125 / 469, Model loss: 0.3963559865951538\n",
      "Batch 250 / 469, Model loss: 0.3569331169128418\n",
      "Batch 375 / 469, Model loss: 0.21396146714687347\n",
      "Training lstm-9\n",
      "0-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.3056037425994873\n",
      "Batch 125 / 469, Model loss: 0.7620195746421814\n",
      "Batch 250 / 469, Model loss: 0.43328869342803955\n",
      "Batch 375 / 469, Model loss: 0.27645233273506165\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.2939315140247345\n",
      "Batch 125 / 469, Model loss: 0.3833581209182739\n",
      "Batch 250 / 469, Model loss: 0.32456955313682556\n",
      "Batch 375 / 469, Model loss: 0.20595133304595947\n",
      "1-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.3347549438476562\n",
      "Batch 125 / 469, Model loss: 0.7043811082839966\n",
      "Batch 250 / 469, Model loss: 0.44958925247192383\n",
      "Batch 375 / 469, Model loss: 0.26973757147789\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.28299760818481445\n",
      "Batch 125 / 469, Model loss: 0.37490788102149963\n",
      "Batch 250 / 469, Model loss: 0.35516491532325745\n",
      "Batch 375 / 469, Model loss: 0.209227055311203\n",
      "2-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.2909677028656006\n",
      "Batch 125 / 469, Model loss: 0.7601727843284607\n",
      "Batch 250 / 469, Model loss: 0.4558933675289154\n",
      "Batch 375 / 469, Model loss: 0.29646310210227966\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.30148380994796753\n",
      "Batch 125 / 469, Model loss: 0.38512885570526123\n",
      "Batch 250 / 469, Model loss: 0.3549063801765442\n",
      "Batch 375 / 469, Model loss: 0.219876229763031\n",
      "Training gnn-9\n",
      "0-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.3205416202545166\n",
      "Batch 125 / 469, Model loss: 0.7216728329658508\n",
      "Batch 250 / 469, Model loss: 0.4622124433517456\n",
      "Batch 375 / 469, Model loss: 0.2670588195323944\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.285274475812912\n",
      "Batch 125 / 469, Model loss: 0.37991827726364136\n",
      "Batch 250 / 469, Model loss: 0.3575162887573242\n",
      "Batch 375 / 469, Model loss: 0.20023053884506226\n",
      "1-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.311192274093628\n",
      "Batch 125 / 469, Model loss: 0.7139657735824585\n",
      "Batch 250 / 469, Model loss: 0.4328506290912628\n",
      "Batch 375 / 469, Model loss: 0.27256327867507935\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.29524531960487366\n",
      "Batch 125 / 469, Model loss: 0.3935425579547882\n",
      "Batch 250 / 469, Model loss: 0.3391123116016388\n",
      "Batch 375 / 469, Model loss: 0.2075415700674057\n",
      "2-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.304292917251587\n",
      "Batch 125 / 469, Model loss: 0.7060953378677368\n",
      "Batch 250 / 469, Model loss: 0.44422000646591187\n",
      "Batch 375 / 469, Model loss: 0.26678743958473206\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.29210659861564636\n",
      "Batch 125 / 469, Model loss: 0.3771970570087433\n",
      "Batch 250 / 469, Model loss: 0.34462136030197144\n",
      "Batch 375 / 469, Model loss: 0.19801801443099976\n"
     ]
    }
   ],
   "source": [
    "# Trains optimizer\n",
    "for i in range(num_optimizers):\n",
    "    print(f\"Training lstm-{i}\")\n",
    "    trainLSTM(3, f'trained_model/lstm_optimizer_{i}.pth')\n",
    "\n",
    "    print(f\"Training gnn-{i}\")\n",
    "    trainGNN(3, f'trained_model/gnn_optimizer_{i}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from models.mnist_nets import deep_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches: 469\n"
     ]
    }
   ],
   "source": [
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "print(f\"Number of batches: {len(train_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "model = deep_net(28*28, 10).to(device)\n",
    "model_for_lstm = [deepcopy(model) for _ in range(num_optimizers)]\n",
    "model_for_gnn= [deepcopy(model) for _ in range(num_optimizers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "his_lstm = [[] for _ in range(num_optimizers)]\n",
    "his_gnn = [[] for _ in range(num_optimizers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.optim_nets import lstm_l2o_optimizer\n",
    "from trainUtil.train_with_LSTM import train_with_lstm\n",
    "from models.optim_nets import gnn_l2o_optimizer\n",
    "from trainUtil.train_with_GNN import train_with_GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "loss: 2.299130  [    0/60000]\n",
      "loss: 0.960628  [12800/60000]\n",
      "loss: 0.728162  [25600/60000]\n",
      "loss: 0.647308  [38400/60000]\n",
      "loss: 0.671008  [51200/60000]\n",
      "Epoch: 2\n",
      "loss: 0.458879  [    0/60000]\n",
      "loss: 0.514302  [12800/60000]\n",
      "loss: 0.492290  [25600/60000]\n",
      "loss: 0.486229  [38400/60000]\n",
      "loss: 0.608186  [51200/60000]\n",
      "Epoch: 3\n",
      "loss: 0.343951  [    0/60000]\n",
      "loss: 0.459757  [12800/60000]\n",
      "loss: 0.405401  [25600/60000]\n",
      "loss: 0.427923  [38400/60000]\n",
      "loss: 0.558305  [51200/60000]\n",
      "Epoch: 4\n",
      "loss: 0.306631  [    0/60000]\n",
      "loss: 0.404669  [12800/60000]\n",
      "loss: 0.353651  [25600/60000]\n",
      "loss: 0.396305  [38400/60000]\n",
      "loss: 0.524934  [51200/60000]\n",
      "Epoch: 1\n",
      "loss: 2.299130  [    0/60000]\n",
      "loss: 0.993157  [12800/60000]\n",
      "loss: 0.740946  [25600/60000]\n",
      "loss: 0.659048  [38400/60000]\n",
      "loss: 0.680129  [51200/60000]\n",
      "Epoch: 2\n",
      "loss: 0.469412  [    0/60000]\n",
      "loss: 0.512875  [12800/60000]\n",
      "loss: 0.498419  [25600/60000]\n",
      "loss: 0.493098  [38400/60000]\n",
      "loss: 0.608554  [51200/60000]\n",
      "Epoch: 3\n",
      "loss: 0.348714  [    0/60000]\n",
      "loss: 0.473401  [12800/60000]\n",
      "loss: 0.416397  [25600/60000]\n",
      "loss: 0.436552  [38400/60000]\n",
      "loss: 0.555107  [51200/60000]\n",
      "Epoch: 4\n",
      "loss: 0.303585  [    0/60000]\n",
      "loss: 0.411152  [12800/60000]\n",
      "loss: 0.360875  [25600/60000]\n",
      "loss: 0.404511  [38400/60000]\n",
      "loss: 0.521653  [51200/60000]\n",
      "Epoch: 1\n",
      "loss: 2.299130  [    0/60000]\n",
      "loss: 0.980674  [12800/60000]\n",
      "loss: 0.735772  [25600/60000]\n",
      "loss: 0.654414  [38400/60000]\n",
      "loss: 0.677503  [51200/60000]\n",
      "Epoch: 2\n",
      "loss: 0.464388  [    0/60000]\n",
      "loss: 0.510426  [12800/60000]\n",
      "loss: 0.494171  [25600/60000]\n",
      "loss: 0.490608  [38400/60000]\n",
      "loss: 0.604997  [51200/60000]\n",
      "Epoch: 3\n",
      "loss: 0.346776  [    0/60000]\n",
      "loss: 0.466336  [12800/60000]\n",
      "loss: 0.410187  [25600/60000]\n",
      "loss: 0.432438  [38400/60000]\n",
      "loss: 0.560024  [51200/60000]\n",
      "Epoch: 4\n",
      "loss: 0.306252  [    0/60000]\n",
      "loss: 0.404363  [12800/60000]\n",
      "loss: 0.360684  [25600/60000]\n",
      "loss: 0.401086  [38400/60000]\n",
      "loss: 0.519400  [51200/60000]\n",
      "Epoch: 1\n",
      "loss: 2.299130  [    0/60000]\n",
      "loss: 0.993137  [12800/60000]\n",
      "loss: 0.741805  [25600/60000]\n",
      "loss: 0.660188  [38400/60000]\n",
      "loss: 0.678984  [51200/60000]\n",
      "Epoch: 2\n",
      "loss: 0.469937  [    0/60000]\n",
      "loss: 0.512549  [12800/60000]\n",
      "loss: 0.498874  [25600/60000]\n",
      "loss: 0.493830  [38400/60000]\n",
      "loss: 0.607395  [51200/60000]\n",
      "Epoch: 3\n",
      "loss: 0.346732  [    0/60000]\n",
      "loss: 0.480823  [12800/60000]\n",
      "loss: 0.417922  [25600/60000]\n",
      "loss: 0.434977  [38400/60000]\n",
      "loss: 0.551400  [51200/60000]\n",
      "Epoch: 4\n",
      "loss: 0.305048  [    0/60000]\n",
      "loss: 0.411186  [12800/60000]\n",
      "loss: 0.364255  [25600/60000]\n",
      "loss: 0.402407  [38400/60000]\n",
      "loss: 0.518987  [51200/60000]\n",
      "Epoch: 1\n",
      "loss: 2.299130  [    0/60000]\n",
      "loss: 0.932732  [12800/60000]\n",
      "loss: 0.720255  [25600/60000]\n",
      "loss: 0.635777  [38400/60000]\n",
      "loss: 0.662495  [51200/60000]\n",
      "Epoch: 2\n",
      "loss: 0.453672  [    0/60000]\n",
      "loss: 0.514318  [12800/60000]\n",
      "loss: 0.486053  [25600/60000]\n",
      "loss: 0.480441  [38400/60000]\n",
      "loss: 0.596937  [51200/60000]\n",
      "Epoch: 3\n",
      "loss: 0.345184  [    0/60000]\n",
      "loss: 0.452590  [12800/60000]\n",
      "loss: 0.397318  [25600/60000]\n",
      "loss: 0.425169  [38400/60000]\n",
      "loss: 0.551815  [51200/60000]\n",
      "Epoch: 4\n",
      "loss: 0.299437  [    0/60000]\n",
      "loss: 0.394210  [12800/60000]\n",
      "loss: 0.345398  [25600/60000]\n",
      "loss: 0.393478  [38400/60000]\n",
      "loss: 0.517968  [51200/60000]\n",
      "Epoch: 1\n",
      "loss: 2.299130  [    0/60000]\n",
      "loss: 0.987019  [12800/60000]\n",
      "loss: 0.739268  [25600/60000]\n",
      "loss: 0.657082  [38400/60000]\n",
      "loss: 0.678302  [51200/60000]\n",
      "Epoch: 2\n",
      "loss: 0.467759  [    0/60000]\n",
      "loss: 0.510966  [12800/60000]\n",
      "loss: 0.497488  [25600/60000]\n",
      "loss: 0.491916  [38400/60000]\n",
      "loss: 0.603150  [51200/60000]\n",
      "Epoch: 3\n",
      "loss: 0.346267  [    0/60000]\n",
      "loss: 0.482029  [12800/60000]\n",
      "loss: 0.414337  [25600/60000]\n",
      "loss: 0.437201  [38400/60000]\n",
      "loss: 0.552836  [51200/60000]\n",
      "Epoch: 4\n",
      "loss: 0.302606  [    0/60000]\n",
      "loss: 0.410586  [12800/60000]\n",
      "loss: 0.360795  [25600/60000]\n",
      "loss: 0.402646  [38400/60000]\n",
      "loss: 0.520928  [51200/60000]\n",
      "Epoch: 1\n",
      "loss: 2.299130  [    0/60000]\n",
      "loss: 0.897467  [12800/60000]\n",
      "loss: 0.709767  [25600/60000]\n",
      "loss: 0.627657  [38400/60000]\n",
      "loss: 0.651195  [51200/60000]\n",
      "Epoch: 2\n",
      "loss: 0.442443  [    0/60000]\n",
      "loss: 0.516476  [12800/60000]\n",
      "loss: 0.481938  [25600/60000]\n",
      "loss: 0.474944  [38400/60000]\n",
      "loss: 0.592529  [51200/60000]\n",
      "Epoch: 3\n",
      "loss: 0.334392  [    0/60000]\n",
      "loss: 0.440752  [12800/60000]\n",
      "loss: 0.392535  [25600/60000]\n",
      "loss: 0.418380  [38400/60000]\n",
      "loss: 0.550394  [51200/60000]\n",
      "Epoch: 4\n",
      "loss: 0.298505  [    0/60000]\n",
      "loss: 0.389008  [12800/60000]\n",
      "loss: 0.342959  [25600/60000]\n",
      "loss: 0.385529  [38400/60000]\n",
      "loss: 0.503507  [51200/60000]\n",
      "Epoch: 1\n",
      "loss: 2.299130  [    0/60000]\n",
      "loss: 0.975972  [12800/60000]\n",
      "loss: 0.732757  [25600/60000]\n",
      "loss: 0.651175  [38400/60000]\n",
      "loss: 0.671527  [51200/60000]\n",
      "Epoch: 2\n",
      "loss: 0.465306  [    0/60000]\n",
      "loss: 0.512126  [12800/60000]\n",
      "loss: 0.493156  [25600/60000]\n",
      "loss: 0.488881  [38400/60000]\n",
      "loss: 0.600892  [51200/60000]\n",
      "Epoch: 3\n",
      "loss: 0.345211  [    0/60000]\n",
      "loss: 0.461406  [12800/60000]\n",
      "loss: 0.414780  [25600/60000]\n",
      "loss: 0.432189  [38400/60000]\n",
      "loss: 0.551675  [51200/60000]\n",
      "Epoch: 4\n",
      "loss: 0.301748  [    0/60000]\n",
      "loss: 0.402482  [12800/60000]\n",
      "loss: 0.356726  [25600/60000]\n",
      "loss: 0.401873  [38400/60000]\n",
      "loss: 0.511643  [51200/60000]\n",
      "Epoch: 1\n",
      "loss: 2.299130  [    0/60000]\n",
      "loss: 0.960357  [12800/60000]\n",
      "loss: 0.727138  [25600/60000]\n",
      "loss: 0.645490  [38400/60000]\n",
      "loss: 0.669717  [51200/60000]\n",
      "Epoch: 2\n",
      "loss: 0.457625  [    0/60000]\n",
      "loss: 0.511233  [12800/60000]\n",
      "loss: 0.492683  [25600/60000]\n",
      "loss: 0.483498  [38400/60000]\n",
      "loss: 0.599772  [51200/60000]\n",
      "Epoch: 3\n",
      "loss: 0.342981  [    0/60000]\n",
      "loss: 0.465111  [12800/60000]\n",
      "loss: 0.409483  [25600/60000]\n",
      "loss: 0.428769  [38400/60000]\n",
      "loss: 0.550111  [51200/60000]\n",
      "Epoch: 4\n",
      "loss: 0.300335  [    0/60000]\n",
      "loss: 0.405177  [12800/60000]\n",
      "loss: 0.356066  [25600/60000]\n",
      "loss: 0.394163  [38400/60000]\n",
      "loss: 0.508264  [51200/60000]\n",
      "Epoch: 1\n",
      "loss: 2.299130  [    0/60000]\n",
      "loss: 0.944774  [12800/60000]\n",
      "loss: 0.723757  [25600/60000]\n",
      "loss: 0.640859  [38400/60000]\n",
      "loss: 0.664421  [51200/60000]\n",
      "Epoch: 2\n",
      "loss: 0.455595  [    0/60000]\n",
      "loss: 0.514152  [12800/60000]\n",
      "loss: 0.490458  [25600/60000]\n",
      "loss: 0.483650  [38400/60000]\n",
      "loss: 0.596743  [51200/60000]\n",
      "Epoch: 3\n",
      "loss: 0.340792  [    0/60000]\n",
      "loss: 0.455327  [12800/60000]\n",
      "loss: 0.403965  [25600/60000]\n",
      "loss: 0.425431  [38400/60000]\n",
      "loss: 0.546665  [51200/60000]\n",
      "Epoch: 4\n",
      "loss: 0.300564  [    0/60000]\n",
      "loss: 0.397367  [12800/60000]\n",
      "loss: 0.354326  [25600/60000]\n",
      "loss: 0.393642  [38400/60000]\n",
      "loss: 0.506559  [51200/60000]\n"
     ]
    }
   ],
   "source": [
    "# Train with lstm\n",
    "for i in range(num_optimizers):\n",
    "    lstm_optimizer = lstm_l2o_optimizer().to(device)\n",
    "    lstm_optimizer.load_state_dict(torch.load(f\"trained_model/lstm_optimizer_{i}.pth\"))\n",
    "    lstm_optimizer.eval()\n",
    "\n",
    "    his_lstm[i] = train_with_lstm(lstm_optimizer, model_for_lstm[i], train_dataloader, num_epochs=4)\n",
    "    \n",
    "    his_lstm[i] = torch.tensor(his_lstm[i]).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "loss: 2.299130  [    0/60000]\n",
      "loss: 0.915930  [12800/60000]\n",
      "loss: 0.724416  [25600/60000]\n",
      "loss: 0.650199  [38400/60000]\n",
      "loss: 0.684865  [51200/60000]\n",
      "Epoch: 2\n",
      "loss: 0.475864  [    0/60000]\n",
      "loss: 0.535292  [12800/60000]\n",
      "loss: 0.503263  [25600/60000]\n",
      "loss: 0.493938  [38400/60000]\n",
      "loss: 0.629322  [51200/60000]\n",
      "Epoch: 3\n",
      "loss: 0.365058  [    0/60000]\n",
      "loss: 0.495712  [12800/60000]\n",
      "loss: 0.424843  [25600/60000]\n",
      "loss: 0.434835  [38400/60000]\n",
      "loss: 0.563073  [51200/60000]\n",
      "Epoch: 4\n",
      "loss: 0.312132  [    0/60000]\n",
      "loss: 0.443811  [12800/60000]\n",
      "loss: 0.372579  [25600/60000]\n",
      "loss: 0.403704  [38400/60000]\n",
      "loss: 0.507647  [51200/60000]\n",
      "Epoch: 1\n",
      "loss: 2.299130  [    0/60000]\n",
      "loss: 0.903638  [12800/60000]\n",
      "loss: 0.748488  [25600/60000]\n",
      "loss: 0.660462  [38400/60000]\n",
      "loss: 0.771060  [51200/60000]\n",
      "Epoch: 2\n",
      "loss: 0.535284  [    0/60000]\n",
      "loss: 0.580050  [12800/60000]\n",
      "loss: 0.525284  [25600/60000]\n",
      "loss: 0.509216  [38400/60000]\n",
      "loss: 0.750213  [51200/60000]\n",
      "Epoch: 3\n",
      "loss: 0.406515  [    0/60000]\n",
      "loss: 0.498758  [12800/60000]\n",
      "loss: 0.423472  [25600/60000]\n",
      "loss: 0.442513  [38400/60000]\n",
      "loss: 0.652163  [51200/60000]\n",
      "Epoch: 4\n",
      "loss: 0.358175  [    0/60000]\n",
      "loss: 0.448309  [12800/60000]\n",
      "loss: 0.405876  [25600/60000]\n",
      "loss: 0.439977  [38400/60000]\n",
      "loss: 0.546130  [51200/60000]\n",
      "Epoch: 1\n",
      "loss: 2.299130  [    0/60000]\n",
      "loss: 0.929301  [12800/60000]\n",
      "loss: 0.764319  [25600/60000]\n",
      "loss: 0.680438  [38400/60000]\n",
      "loss: 0.795577  [51200/60000]\n",
      "Epoch: 2\n",
      "loss: 0.559780  [    0/60000]\n",
      "loss: 0.598717  [12800/60000]\n",
      "loss: 0.545315  [25600/60000]\n",
      "loss: 0.535811  [38400/60000]\n",
      "loss: 0.753683  [51200/60000]\n",
      "Epoch: 3\n",
      "loss: 0.449867  [    0/60000]\n",
      "loss: 0.559973  [12800/60000]\n",
      "loss: 0.480632  [25600/60000]\n",
      "loss: 0.508887  [38400/60000]\n",
      "loss: 0.626497  [51200/60000]\n",
      "Epoch: 4\n",
      "loss: 0.407766  [    0/60000]\n",
      "loss: 0.461589  [12800/60000]\n",
      "loss: 0.465738  [25600/60000]\n",
      "loss: 0.463862  [38400/60000]\n",
      "loss: 0.568050  [51200/60000]\n",
      "Epoch: 1\n",
      "loss: 2.299130  [    0/60000]\n",
      "loss: 0.891044  [12800/60000]\n",
      "loss: 0.738469  [25600/60000]\n",
      "loss: 0.649854  [38400/60000]\n",
      "loss: 0.747252  [51200/60000]\n",
      "Epoch: 2\n",
      "loss: 0.518212  [    0/60000]\n",
      "loss: 0.568211  [12800/60000]\n",
      "loss: 0.515340  [25600/60000]\n",
      "loss: 0.502064  [38400/60000]\n",
      "loss: 0.729724  [51200/60000]\n",
      "Epoch: 3\n",
      "loss: 0.392732  [    0/60000]\n",
      "loss: 0.492446  [12800/60000]\n",
      "loss: 0.414916  [25600/60000]\n",
      "loss: 0.434181  [38400/60000]\n",
      "loss: 0.623629  [51200/60000]\n",
      "Epoch: 4\n",
      "loss: 0.334600  [    0/60000]\n",
      "loss: 0.433002  [12800/60000]\n",
      "loss: 0.363358  [25600/60000]\n",
      "loss: 0.398437  [38400/60000]\n",
      "loss: 0.534094  [51200/60000]\n",
      "Epoch: 1\n",
      "loss: 2.299130  [    0/60000]\n",
      "loss: 0.954781  [12800/60000]\n",
      "loss: 0.754942  [25600/60000]\n",
      "loss: 0.670292  [38400/60000]\n",
      "loss: 0.759632  [51200/60000]\n",
      "Epoch: 2\n",
      "loss: 0.540275  [    0/60000]\n",
      "loss: 0.577702  [12800/60000]\n",
      "loss: 0.531651  [25600/60000]\n",
      "loss: 0.517732  [38400/60000]\n",
      "loss: 0.750327  [51200/60000]\n",
      "Epoch: 3\n",
      "loss: 0.408562  [    0/60000]\n",
      "loss: 0.507051  [12800/60000]\n",
      "loss: 0.430415  [25600/60000]\n",
      "loss: 0.453873  [38400/60000]\n",
      "loss: 0.627535  [51200/60000]\n",
      "Epoch: 4\n",
      "loss: 0.350349  [    0/60000]\n",
      "loss: 0.448334  [12800/60000]\n",
      "loss: 0.390738  [25600/60000]\n",
      "loss: 0.417764  [38400/60000]\n",
      "loss: 0.544481  [51200/60000]\n",
      "Epoch: 1\n",
      "loss: 2.299130  [    0/60000]\n",
      "loss: 0.864436  [12800/60000]\n",
      "loss: 0.722610  [25600/60000]\n",
      "loss: 0.636322  [38400/60000]\n",
      "loss: 0.700817  [51200/60000]\n",
      "Epoch: 2\n",
      "loss: 0.487597  [    0/60000]\n",
      "loss: 0.569907  [12800/60000]\n",
      "loss: 0.508880  [25600/60000]\n",
      "loss: 0.494481  [38400/60000]\n",
      "loss: 0.671080  [51200/60000]\n",
      "Epoch: 3\n",
      "loss: 0.378803  [    0/60000]\n",
      "loss: 0.494588  [12800/60000]\n",
      "loss: 0.409066  [25600/60000]\n",
      "loss: 0.429148  [38400/60000]\n",
      "loss: 0.582522  [51200/60000]\n",
      "Epoch: 4\n",
      "loss: 0.318295  [    0/60000]\n",
      "loss: 0.439279  [12800/60000]\n",
      "loss: 0.358447  [25600/60000]\n",
      "loss: 0.388614  [38400/60000]\n",
      "loss: 0.518501  [51200/60000]\n",
      "Epoch: 1\n",
      "loss: 2.299130  [    0/60000]\n",
      "loss: 1.133967  [12800/60000]\n",
      "loss: 0.879103  [25600/60000]\n",
      "loss: 0.771266  [38400/60000]\n",
      "loss: 0.917325  [51200/60000]\n",
      "Epoch: 2\n",
      "loss: 0.698715  [    0/60000]\n",
      "loss: 0.716320  [12800/60000]\n",
      "loss: 0.735492  [25600/60000]\n",
      "loss: 0.695655  [38400/60000]\n",
      "loss: 0.839373  [51200/60000]\n",
      "Epoch: 3\n",
      "loss: 0.674473  [    0/60000]\n",
      "loss: 0.686687  [12800/60000]\n",
      "loss: 0.662890  [25600/60000]\n",
      "loss: 0.620909  [38400/60000]\n",
      "loss: 0.825815  [51200/60000]\n",
      "Epoch: 4\n",
      "loss: 0.541569  [    0/60000]\n",
      "loss: 0.646046  [12800/60000]\n",
      "loss: 0.588141  [25600/60000]\n",
      "loss: 0.548084  [38400/60000]\n",
      "loss: 0.783608  [51200/60000]\n",
      "Epoch: 1\n",
      "loss: 2.299130  [    0/60000]\n",
      "loss: 0.931109  [12800/60000]\n",
      "loss: 0.741312  [25600/60000]\n",
      "loss: 0.660232  [38400/60000]\n",
      "loss: 0.731853  [51200/60000]\n",
      "Epoch: 2\n",
      "loss: 0.511384  [    0/60000]\n",
      "loss: 0.562373  [12800/60000]\n",
      "loss: 0.517458  [25600/60000]\n",
      "loss: 0.507014  [38400/60000]\n",
      "loss: 0.707404  [51200/60000]\n",
      "Epoch: 3\n",
      "loss: 0.391996  [    0/60000]\n",
      "loss: 0.503347  [12800/60000]\n",
      "loss: 0.424548  [25600/60000]\n",
      "loss: 0.443748  [38400/60000]\n",
      "loss: 0.622127  [51200/60000]\n",
      "Epoch: 4\n",
      "loss: 0.332020  [    0/60000]\n",
      "loss: 0.438104  [12800/60000]\n",
      "loss: 0.373855  [25600/60000]\n",
      "loss: 0.401903  [38400/60000]\n",
      "loss: 0.535475  [51200/60000]\n",
      "Epoch: 1\n",
      "loss: 2.299130  [    0/60000]\n",
      "loss: 0.867587  [12800/60000]\n",
      "loss: 0.704142  [25600/60000]\n",
      "loss: 0.624988  [38400/60000]\n",
      "loss: 0.650380  [51200/60000]\n",
      "Epoch: 2\n",
      "loss: 0.440448  [    0/60000]\n",
      "loss: 0.527743  [12800/60000]\n",
      "loss: 0.490657  [25600/60000]\n",
      "loss: 0.474168  [38400/60000]\n",
      "loss: 0.596127  [51200/60000]\n",
      "Epoch: 3\n",
      "loss: 0.336929  [    0/60000]\n",
      "loss: 0.450920  [12800/60000]\n",
      "loss: 0.403204  [25600/60000]\n",
      "loss: 0.422805  [38400/60000]\n",
      "loss: 0.544816  [51200/60000]\n",
      "Epoch: 4\n",
      "loss: 0.298229  [    0/60000]\n",
      "loss: 0.395116  [12800/60000]\n",
      "loss: 0.342841  [25600/60000]\n",
      "loss: 0.390484  [38400/60000]\n",
      "loss: 0.495451  [51200/60000]\n",
      "Epoch: 1\n",
      "loss: 2.299130  [    0/60000]\n",
      "loss: 0.852039  [12800/60000]\n",
      "loss: 0.707117  [25600/60000]\n",
      "loss: 0.625399  [38400/60000]\n",
      "loss: 0.670523  [51200/60000]\n",
      "Epoch: 2\n",
      "loss: 0.455779  [    0/60000]\n",
      "loss: 0.534330  [12800/60000]\n",
      "loss: 0.498662  [25600/60000]\n",
      "loss: 0.483042  [38400/60000]\n",
      "loss: 0.634381  [51200/60000]\n",
      "Epoch: 3\n",
      "loss: 0.356018  [    0/60000]\n",
      "loss: 0.464402  [12800/60000]\n",
      "loss: 0.400083  [25600/60000]\n",
      "loss: 0.424176  [38400/60000]\n",
      "loss: 0.554412  [51200/60000]\n",
      "Epoch: 4\n",
      "loss: 0.311299  [    0/60000]\n",
      "loss: 0.414182  [12800/60000]\n",
      "loss: 0.351127  [25600/60000]\n",
      "loss: 0.389821  [38400/60000]\n",
      "loss: 0.490204  [51200/60000]\n"
     ]
    }
   ],
   "source": [
    "# Train with gnn\n",
    "for i in range(num_optimizers):\n",
    "    gnn_optimizer = gnn_l2o_optimizer().to(device)\n",
    "    gnn_optimizer.load_state_dict(torch.load(f\"trained_model/gnn_optimizer_{i}.pth\"))\n",
    "    gnn_optimizer.eval()\n",
    "\n",
    "    his_gnn[i] = train_with_GNN(gnn_optimizer, model_for_gnn[i], train_dataloader, num_epochs=4)\n",
    "    \n",
    "    his_gnn[i] = torch.tensor(his_gnn[i]).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics\n",
    "acc_loss_lstm = [torch.sum(his_lstm[i]) for i in range(num_optimizers)]\n",
    "acc_loss_gnn = [torch.sum(his_gnn[i]) for i in range(num_optimizers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_loss_lstm = torch.tensor(acc_loss_lstm)\n",
    "acc_loss_gnn = torch.tensor(acc_loss_gnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(75.2830)\n",
      "tensor(77.1960)\n",
      "tensor(0.9261)\n",
      "tensor(78.1904)\n"
     ]
    }
   ],
   "source": [
    "print(acc_loss_lstm.min())\n",
    "print(acc_loss_lstm.mean())\n",
    "print(acc_loss_lstm.std())\n",
    "print(acc_loss_lstm.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(75.1924)\n",
      "tensor(80.4617)\n",
      "tensor(7.5496)\n",
      "tensor(100.8942)\n"
     ]
    }
   ],
   "source": [
    "print(acc_loss_gnn.min())\n",
    "print(acc_loss_gnn.mean())\n",
    "print(acc_loss_gnn.std())\n",
    "print(acc_loss_gnn.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.xlabel('Training Steps')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.title('Comparing Training using Different Optimizers')\n",
    "\n",
    "lstm_index = acc_loss_lstm.argmin()\n",
    "gnn_index = acc_loss_gnn.argmin()\n",
    "\n",
    "plt.plot(his_lstm[lstm_index], label=\"L2O-LSTM\")\n",
    "plt.plot(his_gnn[gnn_index], label=\"L2O-GNN\")\n",
    "\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
