{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "This file trains `num_optimizers` LSTM and GNN optimizers, use them to perform certain tasks, and then compare the statistics of their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from trainUtil.trainingLSTM import trainLSTM\n",
    "from trainUtil.trainingGNN import trainGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "num_optimizers = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training lstm-0\n",
      "0-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.3557214736938477\n",
      "Batch 125 / 469, Model loss: 0.6286089420318604\n",
      "Batch 250 / 469, Model loss: 0.42230042815208435\n",
      "Batch 375 / 469, Model loss: 0.2537669241428375\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.29018959403038025\n",
      "Batch 125 / 469, Model loss: 0.41859161853790283\n",
      "Batch 250 / 469, Model loss: 0.36551350355148315\n",
      "Batch 375 / 469, Model loss: 0.27287986874580383\n",
      "1-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.299006938934326\n",
      "Batch 125 / 469, Model loss: 0.5245992541313171\n",
      "Batch 250 / 469, Model loss: 0.36455240845680237\n",
      "Batch 375 / 469, Model loss: 0.23145566880702972\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.24291564524173737\n",
      "Batch 125 / 469, Model loss: 0.3329777121543884\n",
      "Batch 250 / 469, Model loss: 0.2829943597316742\n",
      "Batch 375 / 469, Model loss: 0.18678027391433716\n",
      "2-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.296865940093994\n",
      "Batch 125 / 469, Model loss: 0.8152872323989868\n",
      "Batch 250 / 469, Model loss: 0.44990381598472595\n",
      "Batch 375 / 469, Model loss: 0.2971303164958954\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.2904365360736847\n",
      "Batch 125 / 469, Model loss: 0.38830453157424927\n",
      "Batch 250 / 469, Model loss: 0.35625648498535156\n",
      "Batch 375 / 469, Model loss: 0.21561390161514282\n",
      "Training gnn-0\n",
      "0-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.3327481746673584\n",
      "Batch 125 / 469, Model loss: 0.6318775415420532\n",
      "Batch 250 / 469, Model loss: 0.40209007263183594\n",
      "Batch 375 / 469, Model loss: 0.24803608655929565\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.23501653969287872\n",
      "Batch 125 / 469, Model loss: 0.3502843677997589\n",
      "Batch 250 / 469, Model loss: 0.33829107880592346\n",
      "Batch 375 / 469, Model loss: 0.2071342170238495\n",
      "1-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.3298683166503906\n",
      "Batch 125 / 469, Model loss: 0.871172308921814\n",
      "Batch 250 / 469, Model loss: 0.4222741425037384\n",
      "Batch 375 / 469, Model loss: 0.29131844639778137\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.294400691986084\n",
      "Batch 125 / 469, Model loss: 0.3630765676498413\n",
      "Batch 250 / 469, Model loss: 0.3340897262096405\n",
      "Batch 375 / 469, Model loss: 0.2111411839723587\n",
      "2-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.3354856967926025\n",
      "Batch 125 / 469, Model loss: 0.7769808173179626\n",
      "Batch 250 / 469, Model loss: 0.45144838094711304\n",
      "Batch 375 / 469, Model loss: 0.2859816253185272\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.29768839478492737\n",
      "Batch 125 / 469, Model loss: 0.383820503950119\n",
      "Batch 250 / 469, Model loss: 0.3551889657974243\n",
      "Batch 375 / 469, Model loss: 0.21720191836357117\n",
      "Training lstm-1\n",
      "0-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.3089725971221924\n",
      "Batch 125 / 469, Model loss: 0.7676549553871155\n",
      "Batch 250 / 469, Model loss: 0.4417959153652191\n",
      "Batch 375 / 469, Model loss: 0.28068313002586365\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.31114354729652405\n",
      "Batch 125 / 469, Model loss: 0.37598633766174316\n",
      "Batch 250 / 469, Model loss: 0.3393031358718872\n",
      "Batch 375 / 469, Model loss: 0.20758071541786194\n",
      "1-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.316037893295288\n",
      "Batch 125 / 469, Model loss: 0.8726287484169006\n",
      "Batch 250 / 469, Model loss: 0.4622543454170227\n",
      "Batch 375 / 469, Model loss: 0.27219241857528687\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.3006768226623535\n",
      "Batch 125 / 469, Model loss: 0.3760419487953186\n",
      "Batch 250 / 469, Model loss: 0.34957438707351685\n",
      "Batch 375 / 469, Model loss: 0.20153282582759857\n",
      "2-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.28977108001709\n",
      "Batch 125 / 469, Model loss: 0.7700207829475403\n",
      "Batch 250 / 469, Model loss: 0.45429766178131104\n",
      "Batch 375 / 469, Model loss: 0.2718566358089447\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.2842411696910858\n",
      "Batch 125 / 469, Model loss: 0.37517088651657104\n",
      "Batch 250 / 469, Model loss: 0.35891860723495483\n",
      "Batch 375 / 469, Model loss: 0.1983705461025238\n",
      "Training gnn-1\n",
      "0-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.2963573932647705\n",
      "Batch 125 / 469, Model loss: 0.7408007383346558\n",
      "Batch 250 / 469, Model loss: 0.46473556756973267\n",
      "Batch 375 / 469, Model loss: 0.2937389314174652\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.2918950617313385\n",
      "Batch 125 / 469, Model loss: 0.39081642031669617\n",
      "Batch 250 / 469, Model loss: 0.37080419063568115\n",
      "Batch 375 / 469, Model loss: 0.21965184807777405\n",
      "1-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.329406261444092\n",
      "Batch 125 / 469, Model loss: 0.7825015783309937\n",
      "Batch 250 / 469, Model loss: 0.44496071338653564\n",
      "Batch 375 / 469, Model loss: 0.28402429819107056\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.31496113538742065\n",
      "Batch 125 / 469, Model loss: 0.3947559893131256\n",
      "Batch 250 / 469, Model loss: 0.34730905294418335\n",
      "Batch 375 / 469, Model loss: 0.21294358372688293\n",
      "2-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.320254325866699\n",
      "Batch 125 / 469, Model loss: 0.7042170763015747\n",
      "Batch 250 / 469, Model loss: 0.43111661076545715\n",
      "Batch 375 / 469, Model loss: 0.2705470025539398\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.28331705927848816\n",
      "Batch 125 / 469, Model loss: 0.38394513726234436\n",
      "Batch 250 / 469, Model loss: 0.3344038724899292\n",
      "Batch 375 / 469, Model loss: 0.20136089622974396\n",
      "Training lstm-2\n",
      "0-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.2948174476623535\n",
      "Batch 125 / 469, Model loss: 0.8067861795425415\n",
      "Batch 250 / 469, Model loss: 0.45155060291290283\n",
      "Batch 375 / 469, Model loss: 0.28813356161117554\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.2774480879306793\n",
      "Batch 125 / 469, Model loss: 0.3745083808898926\n",
      "Batch 250 / 469, Model loss: 0.34901243448257446\n",
      "Batch 375 / 469, Model loss: 0.21178971230983734\n",
      "1-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.282846450805664\n",
      "Batch 125 / 469, Model loss: 0.7822891473770142\n",
      "Batch 250 / 469, Model loss: 0.43675339221954346\n",
      "Batch 375 / 469, Model loss: 0.2783379554748535\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.2897990047931671\n",
      "Batch 125 / 469, Model loss: 0.383173406124115\n",
      "Batch 250 / 469, Model loss: 0.3572155237197876\n",
      "Batch 375 / 469, Model loss: 0.20688219368457794\n",
      "2-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.3002960681915283\n",
      "Batch 125 / 469, Model loss: 0.9058278799057007\n",
      "Batch 250 / 469, Model loss: 0.47665655612945557\n",
      "Batch 375 / 469, Model loss: 0.29426535964012146\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.3059997260570526\n",
      "Batch 125 / 469, Model loss: 0.40638792514801025\n",
      "Batch 250 / 469, Model loss: 0.3598042130470276\n",
      "Batch 375 / 469, Model loss: 0.20721568167209625\n",
      "Training gnn-2\n",
      "0-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.332275629043579\n",
      "Batch 125 / 469, Model loss: 0.7555601596832275\n",
      "Batch 250 / 469, Model loss: 0.4169022738933563\n",
      "Batch 375 / 469, Model loss: 0.2829956114292145\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.2823735773563385\n",
      "Batch 125 / 469, Model loss: 0.3956776261329651\n",
      "Batch 250 / 469, Model loss: 0.33565038442611694\n",
      "Batch 375 / 469, Model loss: 0.2102229744195938\n",
      "1-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.3327128887176514\n",
      "Batch 125 / 469, Model loss: 0.7608673572540283\n",
      "Batch 250 / 469, Model loss: 0.4351029396057129\n",
      "Batch 375 / 469, Model loss: 0.28712648153305054\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.29722893238067627\n",
      "Batch 125 / 469, Model loss: 0.39751380681991577\n",
      "Batch 250 / 469, Model loss: 0.3452027142047882\n",
      "Batch 375 / 469, Model loss: 0.21253523230552673\n",
      "2-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.2972469329833984\n",
      "Batch 125 / 469, Model loss: 0.6996668577194214\n",
      "Batch 250 / 469, Model loss: 0.4442269504070282\n",
      "Batch 375 / 469, Model loss: 0.28100156784057617\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.29146137833595276\n",
      "Batch 125 / 469, Model loss: 0.4027240574359894\n",
      "Batch 250 / 469, Model loss: 0.3516225516796112\n",
      "Batch 375 / 469, Model loss: 0.2047816663980484\n",
      "Training lstm-3\n",
      "0-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.3118479251861572\n",
      "Batch 125 / 469, Model loss: 0.7957957983016968\n",
      "Batch 250 / 469, Model loss: 0.4558711647987366\n",
      "Batch 375 / 469, Model loss: 0.284602552652359\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.29030466079711914\n",
      "Batch 125 / 469, Model loss: 0.3928905427455902\n",
      "Batch 250 / 469, Model loss: 0.36075854301452637\n",
      "Batch 375 / 469, Model loss: 0.2090759575366974\n",
      "1-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.298898935317993\n",
      "Batch 125 / 469, Model loss: 0.8199217915534973\n",
      "Batch 250 / 469, Model loss: 0.46199050545692444\n",
      "Batch 375 / 469, Model loss: 0.28604811429977417\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.30151474475860596\n",
      "Batch 125 / 469, Model loss: 0.394869863986969\n",
      "Batch 250 / 469, Model loss: 0.3511844873428345\n",
      "Batch 375 / 469, Model loss: 0.2029111534357071\n",
      "2-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.2915937900543213\n",
      "Batch 125 / 469, Model loss: 0.7663424015045166\n",
      "Batch 250 / 469, Model loss: 0.4507562220096588\n",
      "Batch 375 / 469, Model loss: 0.2906630337238312\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.28755074739456177\n",
      "Batch 125 / 469, Model loss: 0.3818642795085907\n",
      "Batch 250 / 469, Model loss: 0.34505951404571533\n",
      "Batch 375 / 469, Model loss: 0.2066129893064499\n",
      "Training gnn-3\n",
      "0-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.3145651817321777\n",
      "Batch 125 / 469, Model loss: 0.7643250823020935\n",
      "Batch 250 / 469, Model loss: 0.4190613627433777\n",
      "Batch 375 / 469, Model loss: 0.27839720249176025\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.282023549079895\n",
      "Batch 125 / 469, Model loss: 0.41488081216812134\n",
      "Batch 250 / 469, Model loss: 0.332522988319397\n",
      "Batch 375 / 469, Model loss: 0.20314942300319672\n",
      "1-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.31066632270813\n",
      "Batch 125 / 469, Model loss: 0.7801327705383301\n",
      "Batch 250 / 469, Model loss: 0.43310657143592834\n",
      "Batch 375 / 469, Model loss: 0.2956433594226837\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.3063812553882599\n",
      "Batch 125 / 469, Model loss: 0.3942077159881592\n",
      "Batch 250 / 469, Model loss: 0.3452679514884949\n",
      "Batch 375 / 469, Model loss: 0.22177262604236603\n",
      "2-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.318615436553955\n",
      "Batch 125 / 469, Model loss: 0.772781491279602\n",
      "Batch 250 / 469, Model loss: 0.43736201524734497\n",
      "Batch 375 / 469, Model loss: 0.27993836998939514\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.2867743670940399\n",
      "Batch 125 / 469, Model loss: 0.39141011238098145\n",
      "Batch 250 / 469, Model loss: 0.3526016175746918\n",
      "Batch 375 / 469, Model loss: 0.21248406171798706\n",
      "Training lstm-4\n",
      "0-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.288709878921509\n",
      "Batch 125 / 469, Model loss: 0.8023948073387146\n",
      "Batch 250 / 469, Model loss: 0.4496832489967346\n",
      "Batch 375 / 469, Model loss: 0.2788287401199341\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.3017868101596832\n",
      "Batch 125 / 469, Model loss: 0.38140493631362915\n",
      "Batch 250 / 469, Model loss: 0.34254369139671326\n",
      "Batch 375 / 469, Model loss: 0.20666572451591492\n",
      "1-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.3228952884674072\n",
      "Batch 125 / 469, Model loss: 0.8444733619689941\n",
      "Batch 250 / 469, Model loss: 0.4511587917804718\n",
      "Batch 375 / 469, Model loss: 0.2829534113407135\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.28852418065071106\n",
      "Batch 125 / 469, Model loss: 0.36378782987594604\n",
      "Batch 250 / 469, Model loss: 0.3340507745742798\n",
      "Batch 375 / 469, Model loss: 0.2086542397737503\n",
      "2-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.311668872833252\n",
      "Batch 125 / 469, Model loss: 0.7371853590011597\n",
      "Batch 250 / 469, Model loss: 0.43620553612709045\n",
      "Batch 375 / 469, Model loss: 0.274827778339386\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.28346362709999084\n",
      "Batch 125 / 469, Model loss: 0.3652798533439636\n",
      "Batch 250 / 469, Model loss: 0.33408409357070923\n",
      "Batch 375 / 469, Model loss: 0.20772206783294678\n",
      "Training gnn-4\n",
      "0-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.3085432052612305\n",
      "Batch 125 / 469, Model loss: 0.7026321887969971\n",
      "Batch 250 / 469, Model loss: 0.44296473264694214\n",
      "Batch 375 / 469, Model loss: 0.2869434654712677\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.298885703086853\n",
      "Batch 125 / 469, Model loss: 0.4125477075576782\n",
      "Batch 250 / 469, Model loss: 0.34650179743766785\n",
      "Batch 375 / 469, Model loss: 0.21702000498771667\n",
      "1-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.2994542121887207\n",
      "Batch 125 / 469, Model loss: 0.7163777947425842\n",
      "Batch 250 / 469, Model loss: 0.4519282877445221\n",
      "Batch 375 / 469, Model loss: 0.2825769782066345\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.2975919544696808\n",
      "Batch 125 / 469, Model loss: 0.3972378373146057\n",
      "Batch 250 / 469, Model loss: 0.34039127826690674\n",
      "Batch 375 / 469, Model loss: 0.213809996843338\n",
      "2-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.3123350143432617\n",
      "Batch 125 / 469, Model loss: 0.7348950505256653\n",
      "Batch 250 / 469, Model loss: 0.42895790934562683\n",
      "Batch 375 / 469, Model loss: 0.28763484954833984\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.2941950559616089\n",
      "Batch 125 / 469, Model loss: 0.40420445799827576\n",
      "Batch 250 / 469, Model loss: 0.34324732422828674\n",
      "Batch 375 / 469, Model loss: 0.21139825880527496\n",
      "Training lstm-5\n",
      "0-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.3072149753570557\n",
      "Batch 125 / 469, Model loss: 0.7483788728713989\n",
      "Batch 250 / 469, Model loss: 0.43822646141052246\n",
      "Batch 375 / 469, Model loss: 0.2751626670360565\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.2832980751991272\n",
      "Batch 125 / 469, Model loss: 0.37446293234825134\n",
      "Batch 250 / 469, Model loss: 0.34284448623657227\n",
      "Batch 375 / 469, Model loss: 0.20136696100234985\n",
      "1-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.3319737911224365\n",
      "Batch 125 / 469, Model loss: 0.7018125057220459\n",
      "Batch 250 / 469, Model loss: 0.44709306955337524\n",
      "Batch 375 / 469, Model loss: 0.2738930583000183\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.2914091646671295\n",
      "Batch 125 / 469, Model loss: 0.3780460059642792\n",
      "Batch 250 / 469, Model loss: 0.335945188999176\n",
      "Batch 375 / 469, Model loss: 0.20618797838687897\n",
      "2-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.30063533782959\n",
      "Batch 125 / 469, Model loss: 0.6952720284461975\n",
      "Batch 250 / 469, Model loss: 0.43137574195861816\n",
      "Batch 375 / 469, Model loss: 0.27716901898384094\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.28237730264663696\n",
      "Batch 125 / 469, Model loss: 0.3667331337928772\n",
      "Batch 250 / 469, Model loss: 0.3400762379169464\n",
      "Batch 375 / 469, Model loss: 0.2037130445241928\n",
      "Training gnn-5\n",
      "0-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.3005752563476562\n",
      "Batch 125 / 469, Model loss: 0.7460526823997498\n",
      "Batch 250 / 469, Model loss: 0.4563939571380615\n",
      "Batch 375 / 469, Model loss: 0.2829248905181885\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.29572078585624695\n",
      "Batch 125 / 469, Model loss: 0.4011467695236206\n",
      "Batch 250 / 469, Model loss: 0.3521445393562317\n",
      "Batch 375 / 469, Model loss: 0.21555732190608978\n",
      "1-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.2911744117736816\n",
      "Batch 125 / 469, Model loss: 0.731837809085846\n",
      "Batch 250 / 469, Model loss: 0.45328137278556824\n",
      "Batch 375 / 469, Model loss: 0.27415117621421814\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.3012237548828125\n",
      "Batch 125 / 469, Model loss: 0.38461166620254517\n",
      "Batch 250 / 469, Model loss: 0.3680143356323242\n",
      "Batch 375 / 469, Model loss: 0.19639316201210022\n",
      "2-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.298403024673462\n",
      "Batch 125 / 469, Model loss: 0.7632666826248169\n",
      "Batch 250 / 469, Model loss: 0.4538012444972992\n",
      "Batch 375 / 469, Model loss: 0.2754786014556885\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.31276440620422363\n",
      "Batch 125 / 469, Model loss: 0.39399832487106323\n",
      "Batch 250 / 469, Model loss: 0.3520754277706146\n",
      "Batch 375 / 469, Model loss: 0.21184854209423065\n",
      "Training lstm-6\n",
      "0-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.305966854095459\n",
      "Batch 125 / 469, Model loss: 0.810653805732727\n",
      "Batch 250 / 469, Model loss: 0.4528005123138428\n",
      "Batch 375 / 469, Model loss: 0.2929396629333496\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.29532933235168457\n",
      "Batch 125 / 469, Model loss: 0.378397673368454\n",
      "Batch 250 / 469, Model loss: 0.3437029719352722\n",
      "Batch 375 / 469, Model loss: 0.2146185040473938\n",
      "1-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.30130672454834\n",
      "Batch 125 / 469, Model loss: 0.8929177522659302\n",
      "Batch 250 / 469, Model loss: 0.4735110402107239\n",
      "Batch 375 / 469, Model loss: 0.2946646213531494\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.30021098256111145\n",
      "Batch 125 / 469, Model loss: 0.3915784955024719\n",
      "Batch 250 / 469, Model loss: 0.35910600423812866\n",
      "Batch 375 / 469, Model loss: 0.20702588558197021\n",
      "2-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.3279764652252197\n",
      "Batch 125 / 469, Model loss: 0.7826290130615234\n",
      "Batch 250 / 469, Model loss: 0.4577488303184509\n",
      "Batch 375 / 469, Model loss: 0.2855184078216553\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.29036465287208557\n",
      "Batch 125 / 469, Model loss: 0.37330707907676697\n",
      "Batch 250 / 469, Model loss: 0.34362277388572693\n",
      "Batch 375 / 469, Model loss: 0.2160712033510208\n",
      "Training gnn-6\n",
      "0-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.3016676902770996\n",
      "Batch 125 / 469, Model loss: 0.673366367816925\n",
      "Batch 250 / 469, Model loss: 0.4253900647163391\n",
      "Batch 375 / 469, Model loss: 0.2797146141529083\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.2920592427253723\n",
      "Batch 125 / 469, Model loss: 0.39224866032600403\n",
      "Batch 250 / 469, Model loss: 0.34309759736061096\n",
      "Batch 375 / 469, Model loss: 0.2051011025905609\n",
      "1-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.301150321960449\n",
      "Batch 125 / 469, Model loss: 0.7762535810470581\n",
      "Batch 250 / 469, Model loss: 0.4380725026130676\n",
      "Batch 375 / 469, Model loss: 0.2681271731853485\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.29892411828041077\n",
      "Batch 125 / 469, Model loss: 0.39318937063217163\n",
      "Batch 250 / 469, Model loss: 0.32904401421546936\n",
      "Batch 375 / 469, Model loss: 0.19199314713478088\n",
      "2-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.2879958152770996\n",
      "Batch 125 / 469, Model loss: 0.731002151966095\n",
      "Batch 250 / 469, Model loss: 0.444359689950943\n",
      "Batch 375 / 469, Model loss: 0.2845441997051239\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.28960099816322327\n",
      "Batch 125 / 469, Model loss: 0.3887471556663513\n",
      "Batch 250 / 469, Model loss: 0.3418741524219513\n",
      "Batch 375 / 469, Model loss: 0.20592334866523743\n",
      "Training lstm-7\n",
      "0-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.3137662410736084\n",
      "Batch 125 / 469, Model loss: 0.7182524800300598\n",
      "Batch 250 / 469, Model loss: 0.44449564814567566\n",
      "Batch 375 / 469, Model loss: 0.2784353196620941\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.2838636040687561\n",
      "Batch 125 / 469, Model loss: 0.3801477253437042\n",
      "Batch 250 / 469, Model loss: 0.35993000864982605\n",
      "Batch 375 / 469, Model loss: 0.20392416417598724\n",
      "1-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.317584753036499\n",
      "Batch 125 / 469, Model loss: 0.8440988063812256\n",
      "Batch 250 / 469, Model loss: 0.44503480195999146\n",
      "Batch 375 / 469, Model loss: 0.29352855682373047\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.28884103894233704\n",
      "Batch 125 / 469, Model loss: 0.3813156187534332\n",
      "Batch 250 / 469, Model loss: 0.3327619731426239\n",
      "Batch 375 / 469, Model loss: 0.211941659450531\n",
      "2-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.307257652282715\n",
      "Batch 125 / 469, Model loss: 0.68846195936203\n",
      "Batch 250 / 469, Model loss: 0.441368043422699\n",
      "Batch 375 / 469, Model loss: 0.269113153219223\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.2796768844127655\n",
      "Batch 125 / 469, Model loss: 0.38179636001586914\n",
      "Batch 250 / 469, Model loss: 0.3497990667819977\n",
      "Batch 375 / 469, Model loss: 0.2029588520526886\n",
      "Training gnn-7\n",
      "0-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.320974349975586\n",
      "Batch 125 / 469, Model loss: 0.8142883777618408\n",
      "Batch 250 / 469, Model loss: 0.437202125787735\n",
      "Batch 375 / 469, Model loss: 0.28272515535354614\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.29361626505851746\n",
      "Batch 125 / 469, Model loss: 0.3969374895095825\n",
      "Batch 250 / 469, Model loss: 0.3340134918689728\n",
      "Batch 375 / 469, Model loss: 0.2064446210861206\n",
      "1-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.309701681137085\n",
      "Batch 125 / 469, Model loss: 0.7314116358757019\n",
      "Batch 250 / 469, Model loss: 0.4230845272541046\n",
      "Batch 375 / 469, Model loss: 0.2727430760860443\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.27940380573272705\n",
      "Batch 125 / 469, Model loss: 0.36899617314338684\n",
      "Batch 250 / 469, Model loss: 0.3347514867782593\n",
      "Batch 375 / 469, Model loss: 0.19005824625492096\n",
      "2-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.3069241046905518\n",
      "Batch 125 / 469, Model loss: 0.7771358489990234\n",
      "Batch 250 / 469, Model loss: 0.43775278329849243\n",
      "Batch 375 / 469, Model loss: 0.2724236249923706\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.2855314016342163\n",
      "Batch 125 / 469, Model loss: 0.3689509332180023\n",
      "Batch 250 / 469, Model loss: 0.32356297969818115\n",
      "Batch 375 / 469, Model loss: 0.19670820236206055\n",
      "Training lstm-8\n",
      "0-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.2912096977233887\n",
      "Batch 125 / 469, Model loss: 0.8240805268287659\n",
      "Batch 250 / 469, Model loss: 0.44644176959991455\n",
      "Batch 375 / 469, Model loss: 0.2884286940097809\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.29792773723602295\n",
      "Batch 125 / 469, Model loss: 0.38598912954330444\n",
      "Batch 250 / 469, Model loss: 0.34165334701538086\n",
      "Batch 375 / 469, Model loss: 0.2065112143754959\n",
      "1-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.2892794609069824\n",
      "Batch 125 / 469, Model loss: 0.8369821310043335\n",
      "Batch 250 / 469, Model loss: 0.45619693398475647\n",
      "Batch 375 / 469, Model loss: 0.2856385111808777\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.30292993783950806\n",
      "Batch 125 / 469, Model loss: 0.41070815920829773\n",
      "Batch 250 / 469, Model loss: 0.3465200960636139\n",
      "Batch 375 / 469, Model loss: 0.21325469017028809\n",
      "2-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.3155269622802734\n",
      "Batch 125 / 469, Model loss: 0.7921053171157837\n",
      "Batch 250 / 469, Model loss: 0.43383681774139404\n",
      "Batch 375 / 469, Model loss: 0.285856693983078\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.2935774624347687\n",
      "Batch 125 / 469, Model loss: 0.38239455223083496\n",
      "Batch 250 / 469, Model loss: 0.3357974886894226\n",
      "Batch 375 / 469, Model loss: 0.21072915196418762\n",
      "Training gnn-8\n",
      "0-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.30912709236145\n",
      "Batch 125 / 469, Model loss: 0.7395843863487244\n",
      "Batch 250 / 469, Model loss: 0.4393019378185272\n",
      "Batch 375 / 469, Model loss: 0.28376874327659607\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.2886219024658203\n",
      "Batch 125 / 469, Model loss: 0.3837273120880127\n",
      "Batch 250 / 469, Model loss: 0.3230277895927429\n",
      "Batch 375 / 469, Model loss: 0.2113044112920761\n",
      "1-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.2823338508605957\n",
      "Batch 125 / 469, Model loss: 0.7210463881492615\n",
      "Batch 250 / 469, Model loss: 0.4550815522670746\n",
      "Batch 375 / 469, Model loss: 0.27618682384490967\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.2934874892234802\n",
      "Batch 125 / 469, Model loss: 0.3859921991825104\n",
      "Batch 250 / 469, Model loss: 0.3535935878753662\n",
      "Batch 375 / 469, Model loss: 0.20599791407585144\n",
      "2-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.292452812194824\n",
      "Batch 125 / 469, Model loss: 0.8212736248970032\n",
      "Batch 250 / 469, Model loss: 0.46375399827957153\n",
      "Batch 375 / 469, Model loss: 0.2877342104911804\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.30858784914016724\n",
      "Batch 125 / 469, Model loss: 0.3963559865951538\n",
      "Batch 250 / 469, Model loss: 0.3569331169128418\n",
      "Batch 375 / 469, Model loss: 0.21396146714687347\n",
      "Training lstm-9\n",
      "0-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.3056037425994873\n",
      "Batch 125 / 469, Model loss: 0.7620195746421814\n",
      "Batch 250 / 469, Model loss: 0.43328869342803955\n",
      "Batch 375 / 469, Model loss: 0.27645233273506165\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.2939315140247345\n",
      "Batch 125 / 469, Model loss: 0.3833581209182739\n",
      "Batch 250 / 469, Model loss: 0.32456955313682556\n",
      "Batch 375 / 469, Model loss: 0.20595133304595947\n",
      "1-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.3347549438476562\n",
      "Batch 125 / 469, Model loss: 0.7043811082839966\n",
      "Batch 250 / 469, Model loss: 0.44958925247192383\n",
      "Batch 375 / 469, Model loss: 0.26973757147789\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.28299760818481445\n",
      "Batch 125 / 469, Model loss: 0.37490788102149963\n",
      "Batch 250 / 469, Model loss: 0.35516491532325745\n",
      "Batch 375 / 469, Model loss: 0.209227055311203\n",
      "2-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.2909677028656006\n",
      "Batch 125 / 469, Model loss: 0.7601727843284607\n",
      "Batch 250 / 469, Model loss: 0.4558933675289154\n",
      "Batch 375 / 469, Model loss: 0.29646310210227966\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.30148380994796753\n",
      "Batch 125 / 469, Model loss: 0.38512885570526123\n",
      "Batch 250 / 469, Model loss: 0.3549063801765442\n",
      "Batch 375 / 469, Model loss: 0.219876229763031\n",
      "Training gnn-9\n",
      "0-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.3205416202545166\n",
      "Batch 125 / 469, Model loss: 0.7216728329658508\n",
      "Batch 250 / 469, Model loss: 0.4622124433517456\n",
      "Batch 375 / 469, Model loss: 0.2670588195323944\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.285274475812912\n",
      "Batch 125 / 469, Model loss: 0.37991827726364136\n",
      "Batch 250 / 469, Model loss: 0.3575162887573242\n",
      "Batch 375 / 469, Model loss: 0.20023053884506226\n",
      "1-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.311192274093628\n",
      "Batch 125 / 469, Model loss: 0.7139657735824585\n",
      "Batch 250 / 469, Model loss: 0.4328506290912628\n",
      "Batch 375 / 469, Model loss: 0.27256327867507935\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.29524531960487366\n",
      "Batch 125 / 469, Model loss: 0.3935425579547882\n",
      "Batch 250 / 469, Model loss: 0.3391123116016388\n",
      "Batch 375 / 469, Model loss: 0.2075415700674057\n",
      "2-th optimizee\n",
      "Epoch 0\n",
      "Batch 0 / 469, Model loss: 2.304292917251587\n",
      "Batch 125 / 469, Model loss: 0.7060953378677368\n",
      "Batch 250 / 469, Model loss: 0.44422000646591187\n",
      "Batch 375 / 469, Model loss: 0.26678743958473206\n",
      "Epoch 1\n",
      "Batch 0 / 469, Model loss: 0.29210659861564636\n",
      "Batch 125 / 469, Model loss: 0.3771970570087433\n",
      "Batch 250 / 469, Model loss: 0.34462136030197144\n",
      "Batch 375 / 469, Model loss: 0.19801801443099976\n"
     ]
    }
   ],
   "source": [
    "# Trains optimizer\n",
    "for i in range(num_optimizers):\n",
    "    print(f\"Training lstm-{i}\")\n",
    "    trainLSTM(3, f'trained_model/lstm_optimizer_{i}.pth')\n",
    "\n",
    "    print(f\"Training gnn-{i}\")\n",
    "    trainGNN(3, f'trained_model/gnn_optimizer_{i}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from models.mnist_nets import deep_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches: 469\n"
     ]
    }
   ],
   "source": [
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "print(f\"Number of batches: {len(train_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "model = deep_net(28*28, 10).to(device)\n",
    "model_for_lstm = [deepcopy(model) for _ in range(num_optimizers)]\n",
    "model_for_gnn= [deepcopy(model) for _ in range(num_optimizers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "his_lstm = [[] for _ in range(num_optimizers)]\n",
    "his_gnn = [[] for _ in range(num_optimizers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.optim_nets import lstm_l2o_optimizer\n",
    "from trainUtil.train_with_LSTM import train_with_lstm\n",
    "from models.optim_nets import gnn_l2o_optimizer\n",
    "from trainUtil.train_with_GNN import train_with_GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "loss: 2.312834  [    0/60000]\n",
      "loss: 1.033126  [12800/60000]\n",
      "loss: 0.813387  [25600/60000]\n",
      "loss: 0.618208  [38400/60000]\n",
      "loss: 0.693585  [51200/60000]\n",
      "Epoch: 2\n",
      "loss: 0.475306  [    0/60000]\n",
      "loss: 0.511703  [12800/60000]\n",
      "loss: 0.510819  [25600/60000]\n",
      "loss: 0.458271  [38400/60000]\n",
      "loss: 0.626790  [51200/60000]\n",
      "Epoch: 3\n",
      "loss: 0.353419  [    0/60000]\n",
      "loss: 0.470260  [12800/60000]\n",
      "loss: 0.429202  [25600/60000]\n",
      "loss: 0.402769  [38400/60000]\n",
      "loss: 0.574998  [51200/60000]\n",
      "Epoch: 4\n",
      "loss: 0.314953  [    0/60000]\n",
      "loss: 0.422005  [12800/60000]\n",
      "loss: 0.372368  [25600/60000]\n",
      "loss: 0.369298  [38400/60000]\n",
      "loss: 0.535926  [51200/60000]\n",
      "Epoch: 1\n",
      "loss: 2.312834  [    0/60000]\n",
      "loss: 1.058913  [12800/60000]\n",
      "loss: 0.822753  [25600/60000]\n",
      "loss: 0.635545  [38400/60000]\n",
      "loss: 0.701208  [51200/60000]\n",
      "Epoch: 2\n",
      "loss: 0.488929  [    0/60000]\n",
      "loss: 0.515657  [12800/60000]\n",
      "loss: 0.518177  [25600/60000]\n",
      "loss: 0.464680  [38400/60000]\n",
      "loss: 0.627456  [51200/60000]\n",
      "Epoch: 3\n",
      "loss: 0.355565  [    0/60000]\n",
      "loss: 0.478500  [12800/60000]\n",
      "loss: 0.434448  [25600/60000]\n",
      "loss: 0.410733  [38400/60000]\n",
      "loss: 0.579700  [51200/60000]\n",
      "Epoch: 4\n",
      "loss: 0.316083  [    0/60000]\n",
      "loss: 0.430526  [12800/60000]\n",
      "loss: 0.386458  [25600/60000]\n",
      "loss: 0.377464  [38400/60000]\n",
      "loss: 0.536001  [51200/60000]\n",
      "Epoch: 1\n",
      "loss: 2.312834  [    0/60000]\n",
      "loss: 1.047051  [12800/60000]\n",
      "loss: 0.817239  [25600/60000]\n",
      "loss: 0.630628  [38400/60000]\n",
      "loss: 0.695990  [51200/60000]\n",
      "Epoch: 2\n",
      "loss: 0.483559  [    0/60000]\n",
      "loss: 0.512431  [12800/60000]\n",
      "loss: 0.517092  [25600/60000]\n",
      "loss: 0.462712  [38400/60000]\n",
      "loss: 0.631367  [51200/60000]\n",
      "Epoch: 3\n",
      "loss: 0.352331  [    0/60000]\n",
      "loss: 0.478559  [12800/60000]\n",
      "loss: 0.429352  [25600/60000]\n",
      "loss: 0.410282  [38400/60000]\n",
      "loss: 0.580998  [51200/60000]\n",
      "Epoch: 4\n",
      "loss: 0.312842  [    0/60000]\n",
      "loss: 0.428109  [12800/60000]\n",
      "loss: 0.374935  [25600/60000]\n",
      "loss: 0.375548  [38400/60000]\n",
      "loss: 0.532334  [51200/60000]\n",
      "Epoch: 1\n",
      "loss: 2.312834  [    0/60000]\n",
      "loss: 1.058374  [12800/60000]\n",
      "loss: 0.823771  [25600/60000]\n",
      "loss: 0.635992  [38400/60000]\n",
      "loss: 0.699537  [51200/60000]\n",
      "Epoch: 2\n",
      "loss: 0.489592  [    0/60000]\n",
      "loss: 0.517570  [12800/60000]\n",
      "loss: 0.520070  [25600/60000]\n",
      "loss: 0.465199  [38400/60000]\n",
      "loss: 0.626680  [51200/60000]\n",
      "Epoch: 3\n",
      "loss: 0.354591  [    0/60000]\n",
      "loss: 0.478587  [12800/60000]\n",
      "loss: 0.432552  [25600/60000]\n",
      "loss: 0.410331  [38400/60000]\n",
      "loss: 0.579388  [51200/60000]\n",
      "Epoch: 4\n",
      "loss: 0.313430  [    0/60000]\n",
      "loss: 0.432585  [12800/60000]\n",
      "loss: 0.381032  [25600/60000]\n",
      "loss: 0.376733  [38400/60000]\n",
      "loss: 0.538895  [51200/60000]\n",
      "Epoch: 1\n",
      "loss: 2.312834  [    0/60000]\n",
      "loss: 1.012653  [12800/60000]\n",
      "loss: 0.809416  [25600/60000]\n",
      "loss: 0.607054  [38400/60000]\n",
      "loss: 0.691430  [51200/60000]\n",
      "Epoch: 2\n",
      "loss: 0.462012  [    0/60000]\n",
      "loss: 0.510310  [12800/60000]\n",
      "loss: 0.499837  [25600/60000]\n",
      "loss: 0.456614  [38400/60000]\n",
      "loss: 0.628771  [51200/60000]\n",
      "Epoch: 3\n",
      "loss: 0.351302  [    0/60000]\n",
      "loss: 0.469855  [12800/60000]\n",
      "loss: 0.421223  [25600/60000]\n",
      "loss: 0.398816  [38400/60000]\n",
      "loss: 0.575192  [51200/60000]\n",
      "Epoch: 4\n",
      "loss: 0.306531  [    0/60000]\n",
      "loss: 0.413964  [12800/60000]\n",
      "loss: 0.364970  [25600/60000]\n",
      "loss: 0.366775  [38400/60000]\n",
      "loss: 0.525398  [51200/60000]\n",
      "Epoch: 1\n",
      "loss: 2.312834  [    0/60000]\n",
      "loss: 1.052851  [12800/60000]\n",
      "loss: 0.821961  [25600/60000]\n",
      "loss: 0.633455  [38400/60000]\n",
      "loss: 0.699980  [51200/60000]\n",
      "Epoch: 2\n",
      "loss: 0.487295  [    0/60000]\n",
      "loss: 0.515193  [12800/60000]\n",
      "loss: 0.519647  [25600/60000]\n",
      "loss: 0.463279  [38400/60000]\n",
      "loss: 0.629899  [51200/60000]\n",
      "Epoch: 3\n",
      "loss: 0.353607  [    0/60000]\n",
      "loss: 0.475912  [12800/60000]\n",
      "loss: 0.433337  [25600/60000]\n",
      "loss: 0.409374  [38400/60000]\n",
      "loss: 0.579800  [51200/60000]\n",
      "Epoch: 4\n",
      "loss: 0.313105  [    0/60000]\n",
      "loss: 0.432507  [12800/60000]\n",
      "loss: 0.379279  [25600/60000]\n",
      "loss: 0.377223  [38400/60000]\n",
      "loss: 0.538550  [51200/60000]\n",
      "Epoch: 1\n",
      "loss: 2.312834  [    0/60000]\n",
      "loss: 0.983665  [12800/60000]\n",
      "loss: 0.824478  [25600/60000]\n",
      "loss: 0.591473  [38400/60000]\n",
      "loss: 0.680832  [51200/60000]\n",
      "Epoch: 2\n",
      "loss: 0.451171  [    0/60000]\n",
      "loss: 0.508626  [12800/60000]\n",
      "loss: 0.493805  [25600/60000]\n",
      "loss: 0.449780  [38400/60000]\n",
      "loss: 0.626953  [51200/60000]\n",
      "Epoch: 3\n",
      "loss: 0.345581  [    0/60000]\n",
      "loss: 0.463847  [12800/60000]\n",
      "loss: 0.417317  [25600/60000]\n",
      "loss: 0.394467  [38400/60000]\n",
      "loss: 0.561192  [51200/60000]\n",
      "Epoch: 4\n",
      "loss: 0.301302  [    0/60000]\n",
      "loss: 0.403144  [12800/60000]\n",
      "loss: 0.353327  [25600/60000]\n",
      "loss: 0.362322  [38400/60000]\n",
      "loss: 0.522124  [51200/60000]\n",
      "Epoch: 1\n",
      "loss: 2.312834  [    0/60000]\n",
      "loss: 1.043239  [12800/60000]\n",
      "loss: 0.817686  [25600/60000]\n",
      "loss: 0.627819  [38400/60000]\n",
      "loss: 0.694653  [51200/60000]\n",
      "Epoch: 2\n",
      "loss: 0.481893  [    0/60000]\n",
      "loss: 0.512409  [12800/60000]\n",
      "loss: 0.514460  [25600/60000]\n",
      "loss: 0.463011  [38400/60000]\n",
      "loss: 0.628540  [51200/60000]\n",
      "Epoch: 3\n",
      "loss: 0.351358  [    0/60000]\n",
      "loss: 0.476783  [12800/60000]\n",
      "loss: 0.427598  [25600/60000]\n",
      "loss: 0.405885  [38400/60000]\n",
      "loss: 0.581856  [51200/60000]\n",
      "Epoch: 4\n",
      "loss: 0.308898  [    0/60000]\n",
      "loss: 0.422843  [12800/60000]\n",
      "loss: 0.374131  [25600/60000]\n",
      "loss: 0.370624  [38400/60000]\n",
      "loss: 0.537162  [51200/60000]\n",
      "Epoch: 1\n",
      "loss: 2.312834  [    0/60000]\n",
      "loss: 1.029708  [12800/60000]\n",
      "loss: 0.814967  [25600/60000]\n",
      "loss: 0.616840  [38400/60000]\n",
      "loss: 0.693737  [51200/60000]\n",
      "Epoch: 2\n",
      "loss: 0.473875  [    0/60000]\n",
      "loss: 0.513025  [12800/60000]\n",
      "loss: 0.514175  [25600/60000]\n",
      "loss: 0.458947  [38400/60000]\n",
      "loss: 0.631543  [51200/60000]\n",
      "Epoch: 3\n",
      "loss: 0.355111  [    0/60000]\n",
      "loss: 0.472325  [12800/60000]\n",
      "loss: 0.431694  [25600/60000]\n",
      "loss: 0.405098  [38400/60000]\n",
      "loss: 0.577617  [51200/60000]\n",
      "Epoch: 4\n",
      "loss: 0.310403  [    0/60000]\n",
      "loss: 0.423429  [12800/60000]\n",
      "loss: 0.368788  [25600/60000]\n",
      "loss: 0.368303  [38400/60000]\n",
      "loss: 0.540572  [51200/60000]\n",
      "Epoch: 1\n",
      "loss: 2.312834  [    0/60000]\n",
      "loss: 1.018086  [12800/60000]\n",
      "loss: 0.813658  [25600/60000]\n",
      "loss: 0.610851  [38400/60000]\n",
      "loss: 0.692576  [51200/60000]\n",
      "Epoch: 2\n",
      "loss: 0.467784  [    0/60000]\n",
      "loss: 0.508692  [12800/60000]\n",
      "loss: 0.507334  [25600/60000]\n",
      "loss: 0.454880  [38400/60000]\n",
      "loss: 0.629477  [51200/60000]\n",
      "Epoch: 3\n",
      "loss: 0.352702  [    0/60000]\n",
      "loss: 0.468414  [12800/60000]\n",
      "loss: 0.424009  [25600/60000]\n",
      "loss: 0.401291  [38400/60000]\n",
      "loss: 0.573871  [51200/60000]\n",
      "Epoch: 4\n",
      "loss: 0.310058  [    0/60000]\n",
      "loss: 0.415536  [12800/60000]\n",
      "loss: 0.367732  [25600/60000]\n",
      "loss: 0.368698  [38400/60000]\n",
      "loss: 0.538893  [51200/60000]\n"
     ]
    }
   ],
   "source": [
    "# Train with lstm\n",
    "for i in range(num_optimizers):\n",
    "    lstm_optimizer = lstm_l2o_optimizer().to(device)\n",
    "    lstm_optimizer.load_state_dict(torch.load(f\"trained_model/lstm_optimizer_{i}.pth\"))\n",
    "    lstm_optimizer.eval()\n",
    "\n",
    "    his_lstm[i] = train_with_lstm(lstm_optimizer, model_for_lstm[i], train_dataloader, num_epochs=4)\n",
    "    \n",
    "    his_lstm[i] = torch.tensor(his_lstm[i]).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "loss: 2.312834  [    0/60000]\n",
      "loss: 0.993786  [12800/60000]\n",
      "loss: 0.833759  [25600/60000]\n",
      "loss: 0.608364  [38400/60000]\n",
      "loss: 0.704379  [51200/60000]\n",
      "Epoch: 2\n",
      "loss: 0.481997  [    0/60000]\n",
      "loss: 0.526450  [12800/60000]\n",
      "loss: 0.515633  [25600/60000]\n",
      "loss: 0.464227  [38400/60000]\n",
      "loss: 0.643071  [51200/60000]\n",
      "Epoch: 3\n",
      "loss: 0.372474  [    0/60000]\n",
      "loss: 0.481195  [12800/60000]\n",
      "loss: 0.434858  [25600/60000]\n",
      "loss: 0.406998  [38400/60000]\n",
      "loss: 0.567365  [51200/60000]\n",
      "Epoch: 4\n",
      "loss: 0.327298  [    0/60000]\n",
      "loss: 0.416480  [12800/60000]\n",
      "loss: 0.371144  [25600/60000]\n",
      "loss: 0.384361  [38400/60000]\n",
      "loss: 0.515720  [51200/60000]\n",
      "Epoch: 1\n",
      "loss: 2.312834  [    0/60000]\n",
      "loss: 0.991756  [12800/60000]\n",
      "loss: 0.867712  [25600/60000]\n",
      "loss: 0.630499  [38400/60000]\n",
      "loss: 0.770293  [51200/60000]\n",
      "Epoch: 2\n",
      "loss: 0.561173  [    0/60000]\n",
      "loss: 0.589883  [12800/60000]\n",
      "loss: 0.556245  [25600/60000]\n",
      "loss: 0.519904  [38400/60000]\n",
      "loss: 0.690216  [51200/60000]\n",
      "Epoch: 3\n",
      "loss: 0.420815  [    0/60000]\n",
      "loss: 0.506180  [12800/60000]\n",
      "loss: 0.429999  [25600/60000]\n",
      "loss: 0.467299  [38400/60000]\n",
      "loss: 0.581595  [51200/60000]\n",
      "Epoch: 4\n",
      "loss: 0.352187  [    0/60000]\n",
      "loss: 0.451467  [12800/60000]\n",
      "loss: 0.381925  [25600/60000]\n",
      "loss: 0.430677  [38400/60000]\n",
      "loss: 0.523419  [51200/60000]\n",
      "Epoch: 1\n",
      "loss: 2.312834  [    0/60000]\n",
      "loss: 1.010188  [12800/60000]\n",
      "loss: 0.870847  [25600/60000]\n",
      "loss: 0.649562  [38400/60000]\n",
      "loss: 0.811414  [51200/60000]\n",
      "Epoch: 2\n",
      "loss: 0.588200  [    0/60000]\n",
      "loss: 0.589942  [12800/60000]\n",
      "loss: 0.571402  [25600/60000]\n",
      "loss: 0.528700  [38400/60000]\n",
      "loss: 0.706544  [51200/60000]\n",
      "Epoch: 3\n",
      "loss: 0.416867  [    0/60000]\n",
      "loss: 0.509393  [12800/60000]\n",
      "loss: 0.443239  [25600/60000]\n",
      "loss: 0.471550  [38400/60000]\n",
      "loss: 0.596549  [51200/60000]\n",
      "Epoch: 4\n",
      "loss: 0.376030  [    0/60000]\n",
      "loss: 0.488077  [12800/60000]\n",
      "loss: 0.451379  [25600/60000]\n",
      "loss: 0.462549  [38400/60000]\n",
      "loss: 0.583050  [51200/60000]\n",
      "Epoch: 1\n",
      "loss: 2.312834  [    0/60000]\n",
      "loss: 0.980199  [12800/60000]\n",
      "loss: 0.877671  [25600/60000]\n",
      "loss: 0.617652  [38400/60000]\n",
      "loss: 0.749207  [51200/60000]\n",
      "Epoch: 2\n",
      "loss: 0.539750  [    0/60000]\n",
      "loss: 0.580491  [12800/60000]\n",
      "loss: 0.540825  [25600/60000]\n",
      "loss: 0.507123  [38400/60000]\n",
      "loss: 0.669648  [51200/60000]\n",
      "Epoch: 3\n",
      "loss: 0.414824  [    0/60000]\n",
      "loss: 0.481659  [12800/60000]\n",
      "loss: 0.420617  [25600/60000]\n",
      "loss: 0.448522  [38400/60000]\n",
      "loss: 0.580041  [51200/60000]\n",
      "Epoch: 4\n",
      "loss: 0.348299  [    0/60000]\n",
      "loss: 0.432605  [12800/60000]\n",
      "loss: 0.370171  [25600/60000]\n",
      "loss: 0.423012  [38400/60000]\n",
      "loss: 0.511553  [51200/60000]\n",
      "Epoch: 1\n",
      "loss: 2.312834  [    0/60000]\n",
      "loss: 1.027810  [12800/60000]\n",
      "loss: 0.851361  [25600/60000]\n",
      "loss: 0.643282  [38400/60000]\n",
      "loss: 0.764668  [51200/60000]\n",
      "Epoch: 2\n",
      "loss: 0.563286  [    0/60000]\n",
      "loss: 0.585696  [12800/60000]\n",
      "loss: 0.565312  [25600/60000]\n",
      "loss: 0.524881  [38400/60000]\n",
      "loss: 0.697803  [51200/60000]\n",
      "Epoch: 3\n",
      "loss: 0.427945  [    0/60000]\n",
      "loss: 0.484483  [12800/60000]\n",
      "loss: 0.436398  [25600/60000]\n",
      "loss: 0.460508  [38400/60000]\n",
      "loss: 0.601813  [51200/60000]\n",
      "Epoch: 4\n",
      "loss: 0.360577  [    0/60000]\n",
      "loss: 0.448006  [12800/60000]\n",
      "loss: 0.381324  [25600/60000]\n",
      "loss: 0.428760  [38400/60000]\n",
      "loss: 0.534932  [51200/60000]\n",
      "Epoch: 1\n",
      "loss: 2.312834  [    0/60000]\n",
      "loss: 0.954563  [12800/60000]\n",
      "loss: 0.881398  [25600/60000]\n",
      "loss: 0.597993  [38400/60000]\n",
      "loss: 0.721035  [51200/60000]\n",
      "Epoch: 2\n",
      "loss: 0.492859  [    0/60000]\n",
      "loss: 0.561645  [12800/60000]\n",
      "loss: 0.523144  [25600/60000]\n",
      "loss: 0.480067  [38400/60000]\n",
      "loss: 0.652869  [51200/60000]\n",
      "Epoch: 3\n",
      "loss: 0.388187  [    0/60000]\n",
      "loss: 0.478800  [12800/60000]\n",
      "loss: 0.419928  [25600/60000]\n",
      "loss: 0.424313  [38400/60000]\n",
      "loss: 0.557018  [51200/60000]\n",
      "Epoch: 4\n",
      "loss: 0.332244  [    0/60000]\n",
      "loss: 0.412138  [12800/60000]\n",
      "loss: 0.360521  [25600/60000]\n",
      "loss: 0.396374  [38400/60000]\n",
      "loss: 0.507025  [51200/60000]\n",
      "Epoch: 1\n",
      "loss: 2.312834  [    0/60000]\n",
      "loss: 1.187221  [12800/60000]\n",
      "loss: 0.943497  [25600/60000]\n",
      "loss: 0.772237  [38400/60000]\n",
      "loss: 0.938486  [51200/60000]\n",
      "Epoch: 2\n",
      "loss: 0.724653  [    0/60000]\n",
      "loss: 0.679834  [12800/60000]\n",
      "loss: 0.703328  [25600/60000]\n",
      "loss: 0.643686  [38400/60000]\n",
      "loss: 0.824323  [51200/60000]\n",
      "Epoch: 3\n",
      "loss: 0.650330  [    0/60000]\n",
      "loss: 0.762989  [12800/60000]\n",
      "loss: 0.718503  [25600/60000]\n",
      "loss: 0.637788  [38400/60000]\n",
      "loss: 0.796217  [51200/60000]\n",
      "Epoch: 4\n",
      "loss: 0.504157  [    0/60000]\n",
      "loss: 0.622982  [12800/60000]\n",
      "loss: 0.627578  [25600/60000]\n",
      "loss: 0.550189  [38400/60000]\n",
      "loss: 0.831083  [51200/60000]\n",
      "Epoch: 1\n",
      "loss: 2.312834  [    0/60000]\n",
      "loss: 1.009927  [12800/60000]\n",
      "loss: 0.840415  [25600/60000]\n",
      "loss: 0.624450  [38400/60000]\n",
      "loss: 0.739463  [51200/60000]\n",
      "Epoch: 2\n",
      "loss: 0.526957  [    0/60000]\n",
      "loss: 0.566007  [12800/60000]\n",
      "loss: 0.540621  [25600/60000]\n",
      "loss: 0.506827  [38400/60000]\n",
      "loss: 0.673017  [51200/60000]\n",
      "Epoch: 3\n",
      "loss: 0.410304  [    0/60000]\n",
      "loss: 0.482861  [12800/60000]\n",
      "loss: 0.427887  [25600/60000]\n",
      "loss: 0.445380  [38400/60000]\n",
      "loss: 0.593164  [51200/60000]\n",
      "Epoch: 4\n",
      "loss: 0.350491  [    0/60000]\n",
      "loss: 0.428615  [12800/60000]\n",
      "loss: 0.378516  [25600/60000]\n",
      "loss: 0.411964  [38400/60000]\n",
      "loss: 0.536645  [51200/60000]\n",
      "Epoch: 1\n",
      "loss: 2.312834  [    0/60000]\n",
      "loss: 0.960067  [12800/60000]\n",
      "loss: 0.840537  [25600/60000]\n",
      "loss: 0.584078  [38400/60000]\n",
      "loss: 0.683012  [51200/60000]\n",
      "Epoch: 2\n",
      "loss: 0.449960  [    0/60000]\n",
      "loss: 0.506779  [12800/60000]\n",
      "loss: 0.491046  [25600/60000]\n",
      "loss: 0.445904  [38400/60000]\n",
      "loss: 0.618254  [51200/60000]\n",
      "Epoch: 3\n",
      "loss: 0.349024  [    0/60000]\n",
      "loss: 0.460225  [12800/60000]\n",
      "loss: 0.408038  [25600/60000]\n",
      "loss: 0.390181  [38400/60000]\n",
      "loss: 0.558287  [51200/60000]\n",
      "Epoch: 4\n",
      "loss: 0.297679  [    0/60000]\n",
      "loss: 0.393012  [12800/60000]\n",
      "loss: 0.352070  [25600/60000]\n",
      "loss: 0.360987  [38400/60000]\n",
      "loss: 0.526601  [51200/60000]\n",
      "Epoch: 1\n",
      "loss: 2.312834  [    0/60000]\n",
      "loss: 0.944616  [12800/60000]\n",
      "loss: 0.870650  [25600/60000]\n",
      "loss: 0.584598  [38400/60000]\n",
      "loss: 0.692469  [51200/60000]\n",
      "Epoch: 2\n",
      "loss: 0.460013  [    0/60000]\n",
      "loss: 0.519734  [12800/60000]\n",
      "loss: 0.502668  [25600/60000]\n",
      "loss: 0.455448  [38400/60000]\n",
      "loss: 0.623360  [51200/60000]\n",
      "Epoch: 3\n",
      "loss: 0.363898  [    0/60000]\n",
      "loss: 0.465230  [12800/60000]\n",
      "loss: 0.413606  [25600/60000]\n",
      "loss: 0.398727  [38400/60000]\n",
      "loss: 0.552572  [51200/60000]\n",
      "Epoch: 4\n",
      "loss: 0.315025  [    0/60000]\n",
      "loss: 0.403197  [12800/60000]\n",
      "loss: 0.358281  [25600/60000]\n",
      "loss: 0.375680  [38400/60000]\n",
      "loss: 0.504764  [51200/60000]\n"
     ]
    }
   ],
   "source": [
    "# Train with gnn\n",
    "for i in range(num_optimizers):\n",
    "    gnn_optimizer = gnn_l2o_optimizer().to(device)\n",
    "    gnn_optimizer.load_state_dict(torch.load(f\"trained_model/gnn_optimizer_{i}.pth\"))\n",
    "    gnn_optimizer.eval()\n",
    "\n",
    "    his_gnn[i] = train_with_GNN(gnn_optimizer, model_for_gnn[i], train_dataloader, num_epochs=4)\n",
    "    \n",
    "    his_gnn[i] = torch.tensor(his_gnn[i]).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics\n",
    "acc_loss_lstm = [torch.sum(his_lstm[i]) for i in range(num_optimizers)]\n",
    "acc_loss_gnn = [torch.sum(his_gnn[i]) for i in range(num_optimizers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_loss_lstm = torch.tensor(acc_loss_lstm)\n",
    "acc_loss_gnn = torch.tensor(acc_loss_gnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(76.5173)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_loss_lstm.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(75.9297)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_loss_gnn.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.xlabel('Training Steps')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.title('Comparing Training using Different Optimizers')\n",
    "\n",
    "lstm_index = acc_loss_lstm.argmin()\n",
    "gnn_index = acc_loss_gnn.argmin()\n",
    "\n",
    "plt.plot(his_lstm[lstm_index], label=\"L2O-LSTM\")\n",
    "plt.plot(his_gnn[gnn_index], label=\"L2O-GNN\")\n",
    "\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
